%RL->MDP, Q, SARSA, SMDP, 
%MDP
%Model-based
%dynamic programming
%Dyna
%Model-free
%Q
%SARSA
\chapter{Background}
\label{ch:RL}

In this chapter, we introduce the basic concept of reinforcement learning (RL)
and the Markov decision process (MDP) and semi-Markov decision process (SMDP) formalisms.
We describe the solution methods of MDPs and SMDPs, that include model-free and model-based 
methods. Then we review the concepts and algorithms in hierarchical reinforcement learning (HRL) frameworks. 
We present the brief introduction of previous work in the RL field.
For more comprehensive introduction of MDP, SMDP and RL, please refer to standard text
\cite{howard1960, Puterman94, SuttonIntro, KevinIntro}.
Bertsekas and Tsitsiklis \cite{Neurodynamic} introduce RL from a more theoretic perspective.
A more detailed introduction of HRL can be found in \cite{HRLSurvey}.

\section{Reinforcement Learning}
Reinforcement learning (RL) addresses the problem for an agent to execute a sequence of actions
in a stochastic environment to maximize its rewards. 

In the RL setting, the environment can be modeled in several formalisms.
One of the most commonly adopted formalisms is Markov decision processes (MDPs).
An MDP assumes that the state of the environment is fully observable, and each action takes a single
step to finish. Semi-Markov decision processes (SMDPs) remove the latter assumption and allow
the action to take several time steps. Partially observable Markov decision processes (POMDPs) remove
the former assumption and the agent needs to decide the actions without access to the full state
of the environment.

Regardless of the underlying models of the environment, the objective of an RL method is
to find an optimal policy for the model. MDPs and SMDPs are introduced in Sections \ref{se:MDP} and \ref{se:SMDP}.


We will cover MDPs and SMDPs in detail
in Sections 2.2 and 2.3. POMDPs will be presented more briefly, as the subject of partial
observability is almost (but not completely) orthogonal to the main contributions of this
dissertation.
, but
the MDP is unknown to the agent in the beginning. 
The task of reinforcement learning
consists of nding an optimal policy for the associated MDP. We will therefore
refer to the underlying MDP as the model of the environment (or simply model
if it is clear from the context which meaning of model is intended). The optimal
policy is denoted by  and the corresponding utility function by V .

%The lack of an apriori known model generates a need to sample the MDP to
%gather statistical knowledge about this unknown model. Depending on the kind
%of knowledge that is gathered, a distinction is made between model-based and
%model-free solution techniques.



Reinforcement learning (RL) (Sutton and Barto, 1998) refers to a collection of methods
that allow an agent (a system) to learn how to make good decisions by observing its
own behavior, and improves its actions through a reinforcement mechanism. There are
many formal specifications of this kind of problems that have been developed over the last
fifty years. The most commonly used is the Markov decision processes (MDPs). An
MDP assumes that the agent has full access to the state of the world and each of its actions

%RL methods can be broadly classified into two classes: model-based
%and model-free. Model-based RL learns an effective policy by constructing the model from samples
%and simulating experiences from the model. It generally requires fewer samples to learn the optimal
%policy. However, when the state space is too large, we cannot build the exact model anymore.
%Instead, we have to approximate the model with techniques such as function approximation. Little
%research has been done to address the problem of model-based RL with approximation in an online
%setting.

%TODO: why RL
%TODO: the challenge of RL

%Given a reward function, reinforcement learning algorithms can
%search over possible action space and find a sequence of actions 
%which can maximize the rewards. The reinforcement learning is an ideal choice
%to develop a learning agent for video games.

%In reinforcement learning, the environment can be modeled as an MDP but
%this MDP is unknown to the RL-agent. The task of reinforcement learning
%consists of nding an optimal policy for the associated MDP. We will therefore
%refer to the underlying MDP as the model of the environment (or simply model
%if it is clear from the context which meaning of model is intended). The optimal
%policy is denoted by  and the corresponding utility function by V .

%The lack of an apriori known model generates a need to sample the MDP to
%gather statistical knowledge about this unknown model. Depending on the kind
%of knowledge that is gathered, a distinction is made between model-based and
%model-free solution techniques.


In the RL setting, the environment is modeled as an MDP problem.
However, the MDP is unknown to the RL agent. The objective of reinforcement learning is to 
learn an optimal policy for the MDP. Since the MDP is unknown in the beginning, 
the agent needs to sample the MDP to acquire the knowledge about the MDP.
The learning agent will take actions based on the current state. For the first step, the agent does not know anything about 
the environment, therefore the agent has to choose the first action randomly. After the environment
receives the action, it will provide a reward to the agent as a feedback. The reward can be either
positive or negative. The agent adjusts the value function to maximize the expected reward in the future.
The initial value of value function is 
usually 0, but it is possible to set it to some high enough value to encourage exploration.
It is important to evaluate the value function 
correctly. If some actions with low expected reward are estimated as high, it degrades the
performance of agent.

Reinforcement learning (RL) (Sutton and Barto, 1998) refers to a collection of methods
that allow an agent (a system) to learn how to make good decisions by observing its
own behavior, and improves its actions through a reinforcement mechanism. There are
many formal specifications of this kind of problems that have been developed over the last
fifty years. The most commonly used is the Markov decision processes (MDPs). An
MDP assumes that the agent has full access to the state of the world and each of its actions
takes a single time step. Semi-Markov decision processes (SMDPs) relax the latter
assumption and allow actions that take several time steps. Finally, partially observable
Markov decision processes (POMDPs) relax the former assumption by allowing the agent
to receive observations that do not necessarily reveal the entire state of the environment.
When a problem is modeled using one of the above, the goal of an RL method is to find a
good (possibly optimal) policy for the model. We will cover MDPs and SMDPs in detail
in Sections 2.2 and 2.3. POMDPs will be presented more briefly, as the subject of partial
observability is almost (but not completely) orthogonal to the main contributions of this
dissertation.


The agent can select an action which leads to the highest value of value function. However, 
this strategy does not allow the agent to explore the states which are not visited before.
A better approach is to use $\epsilon$-greedy method. The method allows the agent to abandon the
best action and choose
a random action with a very small probability $\epsilon$. The higher the probability, the more
likely that the agent would explore the new actions. However, if the exploration probability 
is too high, it will increase the time to converge.

After the agent takes an action, the environment will provide a reward and a state to the
agent. The agent then decides an new action for the new state. After several iterations
, the agent will learn a correspondence between the action and state. The correspondence is called 
"policy". 

\section{Markov Decision Processes}

In this work, we address the finite state Markov decision process (MDP) problem:
%\subsection{Model Formulation}
\begin{definition} A Markov decision process is formalized as a tuple $<S, A, P, R>$, where:
\begin{itemize}
    \item $S$ is a finite set of states of the environment.
    \item $A$ is a finite set of actions.
    \item $P:S \times A \times S \rightarrow [0, 1]$ is the transition function which defines a probability distribution over the possible next states.
    \item $R:S \times A \times S \rightarrow \mathbb{R}$ is the reward function which defines the reward after executing a certain action at a certain state.
 \end{itemize}
\end{definition}

Given a state of the environment, a policy $\pi: S \rightarrow A$ dictates what action should be performed at that state. 
The Q-function represents the expected cumulative reward after action $a$ is executed in state $s$ and 
policy $\pi$ is followed thereafter.
The value function $V^{\pi}: S \rightarrow \mathbb{R}$ represents the expected cumulative reward when 
policy $\pi$ is followed from state $s$.

The value function satisfies the Bellman equation:
\begin{equation}
    V^{\pi}(s) = \sum_{s'}P(s'|s, \pi(s))[R(s'|s, \pi(s)) + \gamma V^{\pi}(s')],
    \label{eq:V}
\end{equation}
where $\gamma \in [0, 1]$ is the discount factor which discounts the future reward to the present value.

Similarly, we define the action-value function (or Q-function) as:
\begin{equation}
    Q^{\pi}(s, a) = \sum_{s'}P(s'|s, a)[R(s'|s, a) + \gamma Q^{\pi}(s', \pi(s'))].
    \label{eq:Q}
\end{equation}
The Q-function represents the expected cumulative reward after action $a$ is executed at state $s$ and 
policy $\pi$ is followed thereafter.

%\begin{equation}
    %Q^{\pi}(s, a) = \sum_{s', N}P(s', N|s, a)[R(s', N|s, a) + \gamma^N Q^{\pi}(s', \pi(s'))].
    %\label{eq:SMDPQ}
%\end{equation}
%Now lets us extends action set $A$ to include composite actions.

%The transition function $P$ and $R$ are modified to include the time to accomplish each composite action:
%\begin{equation}
    %R(s, a) = \sum^{\infty}_{k=0} \gamma^k r_k
%\end{equation}

%The value function needs to be modified as:
%\begin{equation}
    %V^{\pi}(s) = \sum_{s'}P(s'|s, \pi(s))[R(s, \pi(s), t) + \gamma^N V^{\pi}(s')],
%\end{equation}
%where $N$ is the number of steps for the action $\pi(s)$ to finish its execution.
%A question arises since we do not know the actual time to finish executing each composite action.
%Let's set $gamma=1$ from now on.
%TODO: (how MaxQ solve it?).

%-------------------------------------------------------------------------


\subsection{Solution Method for MDPs}
\subsubsection{Model-Free Methods}
\subsubsection{Model-Based Methods}
\section{Temporal Difference}
\label{sec:TD}
There are 3 types of reinforcement learning algorithms -- dynamic programming (DP), Monte Carlo 
methods, and temporal-difference (TD). Dynamic programming can compute the optimal policy, but it 
requires a precise model of the environment. In most of the cases, the environment
is too complex to be modeled precisely, and it is not easy to get the complete information about
the environment. On the other hand, it is usually possible to use Monte Carlo method to sample the environment to
get the partial information. 
Like Monte Carlo method, TD uses sampling, therefore it does not require the 
complete model of the environment. TD method is a bootstrapping method, similar to the dynamic 
programming approach, it updates the new value function based on the previous one.

The equation to compute the value function in TD:
\begin{displaymath}
   V(S_t) \leftarrow V(S_t) + \alpha [r_{t+1} + \gamma V(S_{t+1}) - V(S_t)],
\end{displaymath}

where $V(s_t)$ is the value function of the state $s_t$. $V(s_t)$ is the expected reward when
the agent reaches the state $s_t$. $r_{t+1}$ is the reward given to the agent when it chooses
the action at state $s_t$.

\section{Q-Learning}
\label{sec:Q-Learning}
    Q-Learning is an off-policy TD approach. Compared to SARSA, Q-Learning updates
the Q value by the highest value of the next possible state-action, rather than the 
next state-action executed by the agent.  
The Q value is updated by:
\begin{displaymath}
   Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1}+\gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t)],
\end{displaymath}

where $\max_a Q(s_{t+1},a)$ is the highest value of the next possible state-action. 

\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: Q-Learning\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode):\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}

\section{SARSA}
\label{sec:SARSA}
SARSA is a on-policy TD approach. On-policy indicates that it learns from the current policy.
Different from other TD approaches, SARSA updates the Q value of the current state-action from the next state-action.
The Q value is updated by:
\begin{displaymath}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)],
\end{displaymath}
where $Q(s, a)$ is the value function for state-pair, and it is the expected reward when the agent takes
the action $a$ at the state $s$. $\alpha$ is step-wise, which controls the learning rate. 
$\gamma$ is the discount factor.


\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: SARSA\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode):\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a'$ based on $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow a'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}


%\section{}
%SARSA is a on-policy TD approach. On-policy indicates that it learns from the current policy.
%Different from other TD approaches, SARSA updates the Q value of the current state-action from the next state-action.
%The Q value is updated by:
%\begin{displaymath}
    %Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)],
%\end{displaymath}
%where $Q(s, a)$ is the value function for state-pair, and it is the expected reward when the agent takes
%the action $a$ at the state $s$. $\alpha$ is step-wise, which controls the learning rate. 
%$\gamma$ is the discount factor.

%\begin{center}
%\begin{tabular}{@{}lp{6cm}@{}}
%\hline
%Algorithm: Dyna-Q\\
%\hline
%Initialize $Q(s, a)$ and $Model(s, a)$\\
%Repeat (for each episode):\\
%\ \ \ \ Repeat (for each step of episode):\\
%\ \ \ \ \ \ \ \ $s \leftarrow$ the current state\\
%\ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
%\ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
%\ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
%\ \ \ \ \ \ \ \ $Model(s, a) \leftarrow s', r$
%\ \ \ \ \ \ \ \ Repeat $N$ times:
%\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow $ random previously observed state
%\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow $ random action previously taken in $s$
%\ \ \ \ \ \ \ \ \ \ \ \ $s', r \leftarrow Model(s, a)$ 
%\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
%\hline  
%\end{tabular}
%\end{center}

%\section{The Dyna Architecture}
%SARSA is a on-policy TD approach. On-policy indicates that it learns from the current policy.
%Different from other TD approaches, SARSA updates the Q value of the current state-action from the next state-action.
%The Q value is updated by:
%\begin{displaymath}
    %Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)],
%\end{displaymath}
%where $Q(s, a)$ is the value function for state-pair, and it is the expected reward when the agent takes
%the action $a$ at the state $s$. $\alpha$ is step-wise, which controls the learning rate. 
%$\gamma$ is the discount factor.

%\begin{center}
%\begin{tabular}{@{}lp{6cm}@{}}
%\hline
%Algorithm: Dyna-Q\\
%\hline
%Initialize $Q(s, a)$ and $Model(s, a)$\\
%Repeat (for each episode):\\
%\ \ \ \ Repeat (for each step of episode):\\
%\ \ \ \ \ \ \ \ $s \leftarrow$ the current state\\
%\ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
%\ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
%\ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
%\ \ \ \ \ \ \ \ $Model(s, a) \leftarrow s', r$
%\ \ \ \ \ \ \ \ Repeat $N$ times:
%\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow $ random previously observed state
%\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow $ random action previously taken in $s$
%\ \ \ \ \ \ \ \ \ \ \ \ $s', r \leftarrow Model(s, a)$ 
%\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
%\hline  
%\end{tabular}
%\end{center}

%--------------------------------

%The lack of an apriori known model generates a need to sample the MDP to
%gather statistical knowledge about this unknown model. Depending on the kind
%of knowledge that is gathered, a distinction is made between model-based and
%model-free solution techniques.
%2.2.1 Model-Based Solution Techniques
%Although the term model-based reinforcement learning is sometimes used to
%refer to solution methods for problems where the MDP is known, we use modelbased
%RL in the traditional sense where methods are considered that do not
%know the model of the world a priori but that do operate by learning this
%model2. This category of approaches is especially important for applications
%where computation is considered to be cheap and real-world experience costly.
%2Another term for these kind of algorithms is indirect reinforcement learning.
%2.2. SOLUTION METHODS 17
%2.2.1.1 Certainty Equivalent Method
%One straightforward method that falls within this category is the certainty
%equivalent method. The idea is to learn the transition and reward function
%by exploring the environment and keeping statistics about the results of each
%action. Once these functions are learned, a dynamic programming approach can
%be used to compute an optimal policy. Dynamic programming (DP) refers to
%a class of algorithms that are able to compute optimal policies given a perfect
%model of the environment as an MDP. Since this dissertation only deals with
%the reinforcement learning setting of the sequential decision making problem,
%dynamic programming techniques are only briey discussed.
%Central in dynamic programming techniques is the so-called Bellman Equation
%[Bellman, 1965] or dynamic programming equation. This equation arises
%from rewriting the denition of the value function. Assuming the discounted
%cumulative future reward as optimality criterium (Equation 2.1), the following
%Bellman Equation is obtained:
%The solution for this set of equations (there is one equation for every state)
%gives the optimal value function V . Note that this solution is unique, but there
%can be several dierent policies that have the same value function. Usually
%an iterative algorithm is used to compute a solution. The two basic methods
%are value iteration [Bellman, 1957] and policy iteration [Howard, 1960]. Value
%iteration (shown in Algorithm 2.1) uses the Bellman equation to iteratively
%compute the optimal value function. The policy iteration algorithm (shown in
%Algorithm 2.2) interleaves a policy evaluation step, which computes the value
%function for the current policy using Equation 2.2, and a policy improvement
%step which changes the policy by choosing a better action in a certain state
%based on the policy evaluation.
%Algorithm 2.1 Value Iteration
%1:    some small value
%2: V1   initial value function
%3: k   1
%4: repeat
%5: for s 2 S do
%6: Vk+1(s)   maxa
%
%R(s; a) + 
%P
%
%7: end for
%8: k   k + 1
%9: until maximal update smaller than 
%18 CHAPTER 2. REINFORCEMENT LEARNING
%Algorithm 2.2 Policy Iteration
%1: 1   initial policy
%2: k   1
%3: repeat
%4: //Compute the value function of policy k by solving the equations
%5: V k (s) := R(s; k(s)) + 
%P
%6: for s 2 S do
%7: // Policy improvement step
%8: k+1(s)   argmaxa2A
%P
%9: k   k + 1
%10: end for
%The most important drawback of the certainty equivalent method is the
%crisp devision between the learning phase and the acting phase: the agent will
%only start to show intelligent behavior once he has learned a full and perfect
%model. It has furthermore been shown that random exploration might not be
%very ecient to gather information that can build the full model [Koenig and
%Simmons, 1993].
%2.2.1.2 The Dyna Architecture
%A more ecient model-based approach is the Dyna architecture [Sutton, 1991].
%The idea of the Dyna architecture is to use a model-free solution technique
%(such as the ones that will be described in the next section), but at the same
%time learn a model of the environment as in the certainty equivalent method.
%This model can then be used to generate extra experience through simulation.
%There are some other methods that build on this idea. Prioritized sweeping
%[Moore and Atkeson, 1993] for instance does not use a uniform distribution
%when generating extra experience but prioritizes them based on their change
%in values.

%--------------------------------

%SARSA 
%\section{minmax Q-Learning}
%\label{sec:minmax}

    %In two player zero-sum game, it's reasonable to take the action of the opponent into consideration.
%In minmax Q-learning, the Q value is a function of state, the action of player, and the action of opponent.
%The Q value is updated by:
%\begin{displaymath}
    %Q(s_t, a_t, o_t) \leftarrow Q(s_t, a_t, o_t) + \alpha [r_{t+1}+\gamma\max_a min_o Q(s_{t+1}, a, o)-Q(s_t, a_t, o_t)],
%\end{displaymath}

%\begin{center}
%\begin{tabular}{@{}lp{6cm}@{}}
%\hline
%Algorithm: minmax Q-learning\\
%\hline
%\ \ \ Initialize: $Q(s, a, o) \leftarrow 1, V(s) \leftarrow 1$
%\ \ \ Repeat (for each episode)\\
%\ \ \ \ \ \ Initialize $s$\\
%\ \ \ \ \ \ Repeat (for each step of episode):\\
%\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
%\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain opponent action $o$, reward $r$ and next state $s'$ from the environment\\
%\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a, o) \leftarrow Q(s, a, o) + \alpha [r + \gamma max_{a'} min_{o'} Q(s', a', o')-Q(s, a, o)]$\\
%\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
%\hline  
%\end{tabular}
%\end{center}

\section{Semi-Markov Decision Processes}

\section{Hierarchical Reinforcement Learning}

Hierarchical reinforcement learning (HRL) framework assume an MDP problem can be decomposed 
into several smaller MDPs and 
Temporal abstraction is an important technique which has been studied extensively in
clasicial AI and RL fields. The idea of hierarchical reinforcement learning (HRL) 
is to decompose one monolith task into several smaller subtasks. Each subtask is 
responsible to learn a policy of part of original state space. It allows reuse 
the policy for 
Reasoning and learning about temporally extended actions has been studied extensively
in several fields including classical AI, control theory, and RL. In this section, we look at
the historical development of hierarchy and temporal abstraction in classical AI, control,
and RL
Work in HRL has followed three main trends: focusing on
subsets of the state space in a divide-and conquer approach (state space decomposition),
34
grouping sequences or sets of actions together (temporal abstraction), and ignoring differences
between states based on the context (state abstraction). Much of the work falls into
several of these categories.


%average reward
In this chapter, we introduce a general hierarchical reinforcement learning (HRL) framework
for simultaneous learning of policies at multiple levels of hierarchy. Our treatment
builds upon the existing approaches such as HAMs (Parr, 1998), options (Sutton et al.,
1999; Precup, 2000), MAXQ (Dietterich, 2000), and PHAMs (Andre and Russell, 2002;
Andre, 2003), especially the MAXQ value function decomposition. In our framework,
we add three-part value function decomposition (Andre and Russell, 2002) to guarantee
hierarchical optimality, and reward shaping (Ng et al., 1999) to reduce the burden of exploration,
to the MAXQ method. Rather than redundantly explain MAXQ and then our
hierarchical framework, we will present our model and note throughout this chapter where
the key pieces were inspired by or are directly related to Dietterichâ€™s MAXQ work. In the
following chapters, we first extend this framework to the average reward model, then we
generalize it to be applicable to problems with continuous state and/or action spaces, and
finally broaden it to be appropriate for domains with multiple cooperative agents.

The difficulty with using the above methods was that decisions in HRL are no longer
made at synchronous time steps, as is traditionally assumed in RL. Instead, agent makes decision
in epochs of variable length, such as when a distinguishing state is reached (e.g., an
intersection in a robot navigation task), or a subtask is completed (e.g., the elevator arrives
on the first floor). Fortunately, a well-known statistical model is available to treat variable
length actions: the SMDP model described in Section 2.3. Here, state transition dynamics
is specified not only by the state where an action was taken, but also by parameters
specifying the length of time since the action was taken. Early work in RL on the SMDP
model studied extensions of algorithms such as Q-learning to continuous-time (Bradtke and
Duff, 1995; Mahadevan et al., 1997b). The early work on SMDP model was then expanded
to include hierarchical task models over fully or partially specified lower level subtasks,
which led to developing powerful HRL models such as hierarchies of abstract machines
(HAMs) (Parr, 1998), options (Sutton et al., 1999; Precup, 2000), MAXQ (Dietterich,
2000), and programmable HAMs (PHAMs) (Andre and Russell, 2001; Andre, 2003). In
the options model (at least in its simplest form), Sutton et. al. studied how to learn policies
given fully specified policies for executing subtasks. In the HAMs formulation, Parr

showed how hierarchical learning could be achieved even when the policies for lower-level
subtasks were only partially specified. The MAXQ model is one of the first methods to
combine temporal abstraction with state abstraction. It provides a more comprehensive
framework for hierarchical learning where instead of policies for subtasks, the learner is
given pseudo-reward functions. Unlike options and HAMs, MAXQ does not rely directly
on reducing the entire problem to a single SMDP. Instead, a hierarchy of SMDPs is created
whose solutions can be learned simultaneously. The key feature of MAXQ is the decomposed
representation of the value function. Dietterich views each subtask as a separate
MDP, and thus represents the value of a state within that MDP as composed of the reward
for taking an action at that state (which might be composed of many rewards along a trajectory
through a subtask) and the expected reward for completing the subtask. To isolate
the subtask from the calling context, Dietterich uses the notion of a pseudo-reward. At the
terminal states of a subtask, the agent is rewarded according to the pseudo-reward, which
is set a priori by the designer, and does not depend on what happens after leaving the current
subtask. Each subtask can then be treated in isolation from the rest of the problem
with the caveat that the solutions learned are only recursively optimal. Each action in the
recursively optimal policy is optimal with respect to the subtask containing the action, all
descendant subtasks, and the pseudo-reward chosen by the designer of the system. Another
important contribution of Dietterichâ€™s work is the idea that state abstraction can be done
separately on the different components of the value function, which allows one to perform
more abstraction. We investigate the MAXQ framework and its related concepts such as
pseudo-reward, recursive optimality, value function decomposition, and state abstraction in
more details in Chapter 3. In the PHAMs model, Andre and Russell extended HAMs and
presented an agent design language for RL. Andre and Russell (2002) also addressed the
issue of safe state abstraction in HRL. Their method yields state abstraction while maintaining
hierarchical optimality.


\subsection{Taxi Domain}
%Logic of Adaptive Behavior ->Options, SMDP, MAXQ, HORDQ, HAMQ
MAXQ. The MAXQ algorithm (Dietterich, 1998, 2000b,a) can be seen as an extension
of the HSMQ algorithm. It relies on the theory of SMDPs but unlike e.g. the options
framework, it does not rely on reducing the complete problem to a single SMDP. Instead a
hierarchical task decomposition in behaviors is assumed to be given, and learning proceeds
in a way similar to SMDP- or HSMQ-learning. MAXQ learns policies equivalent to HSMQ,
but in addition, it uses a sophisticated value function decomposition to learn these more
134
3.8 ABSTRACTION TYPE V: Hierarchical and Temporal Abstraction
efficiently. The value of a behavior in the context of its calling parent behavior can be
decomposed into i) the reward expected while executing it and ii) the discounted reward
of continuing to execute the parent task after it terminates. Let P be the parent behavior
of B, then
QP(s; B) = IP(s; B) + CP(P; s; B)
where IP(s; B) is the expected total discounted reward that is received while executing
behavior B from initial state s and CP(P; s; B) is the expected total reward of continuing to
execute behavior P after B has terminated, discounted appropriately with respect to the
time spent on executing behavior B.
Pickup
North East
t/source
Put
Putdown
South West
Get
t/destination
Root
Navigate(t)
Figure 3.18: An example of hierarchical abstraction:
the MAXQ task hierarchy (Dietterich,
2000a). The hierarchy constrains the policy space
in the taxi domain. The leaves of the tree contain
basic actions in the domain (such as North) and
the inner nodes represent behaviors that are constructed
using actions and behaviors lower in the
tree. Note that the Navigate behavior is reused
for both getting to the passenger and delivering
him, and also that it is parameterized using the
target location.
The IP(s; B) can be recursively decomposed
into I and C following
IP(s; B) = max
a2AB
QP(s; a)
There are several advantages of this decomposition,
specifically in learning recursively
optimal Q-values. Both the I and C functions
can be represented using different state
space abstractions, allowing for sharing (and
compactness) in the representation of the
value function. See (Dietterich, 2000a) for
more subtle details on this decomposition.
HAMQ. Q-learning with hierarchies of abstract
machines (HAMQ) (Parr and Russell,
1998) learns hierarchically optimal policies,
and uses devices resembling finite-state machines
to implement behaviors. These machines
include an internal state and this state
determines the actions that are taken. Like
the options framework, HAMs are based on
the SMDP model, though the aim here is not
to enlarge the action space with behaviors, but instead to simplify large MDPs by restricting
the policy space. The core idea in HAM is that the policies for the original MDP are
defined as programs which execute based on their own state as well as the state of the
underlying MDP. There are several types of actions. There are primitive actions, actions
that terminate the current behavior and return control to the calling behavior and actions
that call other behaviors. Learning takes place only at choice points where a behavior
must decide which of several internal transitions to make. These choice-points represent
a trade-off between fully hard-coded policies and learned policies. All the finite state machines
representing the behaviors are compiled into one machine where the learning takes
place. Andre and Russell (2001) extended the framework to programmable HAMs, adding
interrupts and the ability to pass parameters, among other things, and later extended the
programming language to ALISP (Andre and Russell, 2002, see also Chapter 7).



