
%RL->MDP, Q, SARSA, SMDP, 
%MDP
%Model-based
%dynamic programming
%Dyna
%Model-free
%Q
%SARSA
\chapter{Reinforcement Learning}
\label{ch:RL}
\section{Model Formulation}

In this work, we address the finite state Markov decision process (MDP) problem:
%\subsection{Model Formulation}
\begin{definition} A Markov decision process is formalized as a tuple $<S, A, P, R>$, where $S$ is a finite set of states of the environment, $A$ is
 a finite set of actions, the transition function $P:S \times A \times S \rightarrow [0, 1]$ defines a
 probability distribution over the possible next states, and the reward function $R:S \times A \rightarrow \mathbb{R}$ defines
 the reward after executing a certain action at a certain state.
\end{definition}

Given a state of the environment, a policy $\pi: S \rightarrow A$ dictates what action should be performed at that state. 
The Q-function represents the expected cumulative reward after action $a$ is executed in state $s$ and 
policy $\pi$ is followed thereafter.
The value function $V^{\pi}: S \rightarrow \mathbb{R}$ represents the expected cumulative reward when 
policy $\pi$ is followed from state $s$.

The value function satisfies the Bellman equation:
\begin{equation}
    V^{\pi}(s) = \sum_{s'}P(s'|s, \pi(s))[R(s, \pi(s)) + \gamma V^{\pi}(s')],
    \label{eq:V}
\end{equation}
where $\gamma \in [0, 1]$ is the discount factor which discounts the future reward to the present value.

Similarly, we define the action-value function (or Q-function) as:
\begin{equation}
    Q^{\pi}(s, a) = \sum_{s'}P(s'|s, a)[R(s'|s, a) + \gamma Q^{\pi}(s', \pi(s'))].
    \label{eq:Q}
\end{equation}
The Q-function represents the expected cumulative reward after action $a$ is executed at state $s$ and 
policy $\pi$ is followed thereafter.

%\begin{equation}
    %Q^{\pi}(s, a) = \sum_{s', N}P(s', N|s, a)[R(s', N|s, a) + \gamma^N Q^{\pi}(s', \pi(s'))].
    %\label{eq:SMDPQ}
%\end{equation}
%Now lets us extends action set $A$ to include composite actions.

%The transition function $P$ and $R$ are modified to include the time to accomplish each composite action:
%\begin{equation}
    %R(s, a) = \sum^{\infty}_{k=0} \gamma^k r_k
%\end{equation}

%The value function needs to be modified as:
%\begin{equation}
    %V^{\pi}(s) = \sum_{s'}P(s'|s, \pi(s))[R(s, \pi(s), t) + \gamma^N V^{\pi}(s')],
%\end{equation}
%where $N$ is the number of steps for the action $\pi(s)$ to finish its execution.
%A question arises since we do not know the actual time to finish executing each composite action.
%Let's set $gamma=1$ from now on.
%TODO: (how MaxQ solve it?).

%In reinforcement learning, the environment can be modeled as an MDP but
%this MDP is unknown to the RL-agent. The task of reinforcement learning
%consists of nding an optimal policy for the associated MDP. We will therefore
%refer to the underlying MDP as the model of the environment (or simply model
%if it is clear from the context which meaning of model is intended). The optimal
%policy is denoted by  and the corresponding utility function by V .

%The lack of an apriori known model generates a need to sample the MDP to
%gather statistical knowledge about this unknown model. Depending on the kind
%of knowledge that is gathered, a distinction is made between model-based and
%model-free solution techniques.
%2.2.1 Model-Based Solution Techniques
%Although the term model-based reinforcement learning is sometimes used to
%refer to solution methods for problems where the MDP is known, we use modelbased
%RL in the traditional sense where methods are considered that do not
%know the model of the world a priori but that do operate by learning this
%model2. This category of approaches is especially important for applications
%where computation is considered to be cheap and real-world experience costly.
%2Another term for these kind of algorithms is indirect reinforcement learning.
%2.2. SOLUTION METHODS 17
%2.2.1.1 Certainty Equivalent Method
%One straightforward method that falls within this category is the certainty
%equivalent method. The idea is to learn the transition and reward function
%by exploring the environment and keeping statistics about the results of each
%action. Once these functions are learned, a dynamic programming approach can
%be used to compute an optimal policy. Dynamic programming (DP) refers to
%a class of algorithms that are able to compute optimal policies given a perfect
%model of the environment as an MDP. Since this dissertation only deals with
%the reinforcement learning setting of the sequential decision making problem,
%dynamic programming techniques are only briey discussed.
%Central in dynamic programming techniques is the so-called Bellman Equation
%[Bellman, 1965] or dynamic programming equation. This equation arises
%from rewriting the denition of the value function. Assuming the discounted
%cumulative future reward as optimality criterium (Equation 2.1), the following
%Bellman Equation is obtained:
%The solution for this set of equations (there is one equation for every state)
%gives the optimal value function V . Note that this solution is unique, but there
%can be several dierent policies that have the same value function. Usually
%an iterative algorithm is used to compute a solution. The two basic methods
%are value iteration [Bellman, 1957] and policy iteration [Howard, 1960]. Value
%iteration (shown in Algorithm 2.1) uses the Bellman equation to iteratively
%compute the optimal value function. The policy iteration algorithm (shown in
%Algorithm 2.2) interleaves a policy evaluation step, which computes the value
%function for the current policy using Equation 2.2, and a policy improvement
%step which changes the policy by choosing a better action in a certain state
%based on the policy evaluation.
%Algorithm 2.1 Value Iteration
%1:    some small value
%2: V1   initial value function
%3: k   1
%4: repeat
%5: for s 2 S do
%6: Vk+1(s)   maxa
%
%R(s; a) + 
%P
%
%7: end for
%8: k   k + 1
%9: until maximal update smaller than 
%18 CHAPTER 2. REINFORCEMENT LEARNING
%Algorithm 2.2 Policy Iteration
%1: 1   initial policy
%2: k   1
%3: repeat
%4: //Compute the value function of policy k by solving the equations
%5: V k (s) := R(s; k(s)) + 
%P
%6: for s 2 S do
%7: // Policy improvement step
%8: k+1(s)   argmaxa2A
%P
%9: k   k + 1
%10: end for
%The most important drawback of the certainty equivalent method is the
%crisp devision between the learning phase and the acting phase: the agent will
%only start to show intelligent behavior once he has learned a full and perfect
%model. It has furthermore been shown that random exploration might not be
%very ecient to gather information that can build the full model [Koenig and
%Simmons, 1993].
%2.2.1.2 The Dyna Architecture
%A more ecient model-based approach is the Dyna architecture [Sutton, 1991].
%The idea of the Dyna architecture is to use a model-free solution technique
%(such as the ones that will be described in the next section), but at the same
%time learn a model of the environment as in the certainty equivalent method.
%This model can then be used to generate extra experience through simulation.
%There are some other methods that build on this idea. Prioritized sweeping
%[Moore and Atkeson, 1993] for instance does not use a uniform distribution
%when generating extra experience but prioritizes them based on their change
%in values.

%-------------------------------------------------------------------------

%Given a reward function, reinforcement learning algorithms can
%search over possible action space and find a sequence of actions 
%which can maximize the rewards. The reinforcement learning is an ideal choice
%to develop a learning agent for video games.

%In reinforcement learning, the environment can be modeled as an MDP but
%this MDP is unknown to the RL-agent. The task of reinforcement learning
%consists of nding an optimal policy for the associated MDP. We will therefore
%refer to the underlying MDP as the model of the environment (or simply model
%if it is clear from the context which meaning of model is intended). The optimal
%policy is denoted by  and the corresponding utility function by V .

%The lack of an apriori known model generates a need to sample the MDP to
%gather statistical knowledge about this unknown model. Depending on the kind
%of knowledge that is gathered, a distinction is made between model-based and
%model-free solution techniques.

%We briefly introduce the basic concept of reinforcement learning in this chapter. 
%For a more completed introduction of reinforcement learning, please refer to
%\cite{SuttonIntro} and \cite{KevinIntro}.

In the reinforcement learning (RL) setting, the environment is modeled as an MDP problem.
However, the MDP is unknown to the RL agent. The objective of reinforcement learning is to 
learn an optimal policy for the MDP. Since the MDP is unknown in the beginning, 
the agent needs to sample the MDP to acquire the knowledge about the MDP.
The learning agent will take actions based on the current state. For the first step, the agent does not know anything about 
the environment, therefore the agent has to choose the first action randomly. After the environment
receives the action, it will provide a reward to the agent as a feedback. The reward can be either
positive or negative. The agent adjusts the value function to maximize the expected reward in the future.
The initial value of value function is 
usually 0, but it is possible to set it to some high enough value to encourage exploration.
It is important to evaluate the value function 
correctly. If some actions with low expected reward are estimated as high, it degrades the
performance of agent.

The agent can select an action which leads to the highest value of value function. However, 
this strategy does not allow the agent to explore the states which are not visited before.
A better approach is to use $\epsilon$-greedy method. The method allows the agent to abandon the
best action and choose
a random action with a very small probability $\epsilon$. The higher the probability, the more
likely that the agent would explore the new actions. However, if the exploration probability 
is too high, it will increase the time to converge.

After the agent takes an action, the environment will provide a reward and a state to the
agent. The agent then decides an new action for the new state. After several iterations
, the agent will learn a correspondence between the action and state. The correspondence is called 
"policy". 

\section{Temporal Difference}
\label{sec:TD}
There are 3 types of reinforcement learning algorithms -- dynamic programming (DP), Monte Carlo 
methods, and temporal-difference (TD). Dynamic programming can compute the optimal policy, but it 
requires a precise model of the environment. In most of the cases, the environment
is too complex to be modeled precisely, and it is not easy to get the complete information about
the environment. On the other hand, it is usually possible to use Monte Carlo method to sample the environment to
get the partial information. 
Like Monte Carlo method, TD uses sampling, therefore it does not require the 
complete model of the environment. TD method is a bootstrapping method, similar to the dynamic 
programming approach, it updates the new value function based on the previous one.

The equation to compute the value function in TD:
\begin{displaymath}
   V(S_t) \leftarrow V(S_t) + \alpha [r_{t+1} + \gamma V(S_{t+1}) - V(S_t)],
\end{displaymath}

where $V(s_t)$ is the value function of the state $s_t$. $V(s_t)$ is the expected reward when
the agent reaches the state $s_t$. $r_{t+1}$ is the reward given to the agent when it chooses
the action at state $s_t$.

\section{Q-Learning}
\label{sec:Q-Learning}
    Q-Learning is an off-policy TD approach. Compared to SARSA, Q-Learning updates
the Q value by the highest value of the next possible state-action, rather than the 
next state-action executed by the agent.  
The Q value is updated by:
\begin{displaymath}
   Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1}+\gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t)],
\end{displaymath}

where $\max_a Q(s_{t+1},a)$ is the highest value of the next possible state-action. 

\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: Q-Learning\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}

\section{SARSA}
\label{sec:SARSA}
SARSA is a on-policy TD approach. On-policy indicates that it learns from the current policy.
Different from other TD approaches, SARSA updates the Q value of the current state-action from the next state-action.
The Q value is updated by:
\begin{displaymath}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)],
\end{displaymath}
where $Q(s, a)$ is the value function for state-pair, and it is the expected reward when the agent takes
the action $a$ at the state $s$. $\alpha$ is step-wise, which controls the learning rate. 
$\gamma$ is the discount factor.


\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: SARSA\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a'$ based on $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow a'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}

%SARSA 
%\section{minmax Q-Learning}
%\label{sec:minmax}

    %In two player zero-sum game, it's reasonable to take the action of the opponent into consideration.
%In minmax Q-learning, the Q value is a function of state, the action of player, and the action of opponent.
%The Q value is updated by:
%\begin{displaymath}
    %Q(s_t, a_t, o_t) \leftarrow Q(s_t, a_t, o_t) + \alpha [r_{t+1}+\gamma\max_a min_o Q(s_{t+1}, a, o)-Q(s_t, a_t, o_t)],
%\end{displaymath}

%\begin{center}
%\begin{tabular}{@{}lp{6cm}@{}}
%\hline
%Algorithm: minmax Q-learning\\
%\hline
%\ \ \ Initialize: $Q(s, a, o) \leftarrow 1, V(s) \leftarrow 1$
%\ \ \ Repeat (for each episode)\\
%\ \ \ \ \ \ Initialize $s$\\
%\ \ \ \ \ \ Repeat (for each step of episode):\\
%\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
%\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain opponent action $o$, reward $r$ and next state $s'$ from the environment\\
%\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a, o) \leftarrow Q(s, a, o) + \alpha [r + \gamma max_{a'} min_{o'} Q(s', a', o')-Q(s, a, o)]$\\
%\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
%\hline  
%\end{tabular}
%\end{center}


