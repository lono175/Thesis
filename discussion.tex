\chapter{Discussion}
In this work, we propose an approach to combine the approximate model-based method with the
model-free method (HORDQ) under the HRL framework. We are able to show that our approach
can learn the optimal policy even when the model is biased. Furthermore, we show that the optimality
is guaranteed for any subtask policy as long as the subtask does not belong to the total leaf
cover of given hierarchy. Our result suggests the possibility incorporating a wide range of planning
techniques such as STRIPS [11] or object-oriented RL [16] into the HRL framework without the
loss of optimality. One future direction is to investigate the applicability of combining our method
with such techniques.

