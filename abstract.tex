%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

%Introduction
%RL->MDP, Q, SARSA, SMDP, 
%MAXQ (include the algorithm), hierarchical optimal vs recursive optimal, HORDQ, Relational,  Model-based RL (cite Peter Stone's Model based approach and a brief intro of it, include the algorithm)
%the problem of HO
%Optimal planning, model the differences
%scale to large problems
%The power of model-based RL and relational approach--> use a maze to illustrate this(model the difference)
%Linear SARSA
%Super Mario

%Motivation:
%1. scale to large problem with small number of samples

%chpater 2: 
%1. MaxQ

%chapter 3:
%1. The power of model based approach--> one wall sample and one pincecess can solve the whole maze problem
   %1.1. show the effectiveness
   %1.2. scaling up--> a noval biased model-based approach (show that it is the only way)
   %1.3. the necessity of biased approach
   %1.4. the choice between planning variable and not
   %1.5. wrong decision variable causes disater (decision)

%chapter 4:
%1. the three optimality ()
%2. Combined with hierarchical optimal RL
%3. why we need leaf cover
%4. the convergence for online approach
%5. the offiline one
%6. Respect the hierarchy

%chpater 5 scaling it up
%1. Better generalization: relational rl
%1.1. the power of model the difference
%2. Reduce training time: transfer learning
%3. case study: Super Mario

%chapter 6 future work
%1. make it hierarchical optimal (add only put down to get passenger subtask in taxi domain)
%2. A general way to combine any approximated model-based approach and can still achieve optimality
\chapter{Abstract}

Model-based reinforcement learning methods make efficient use of samples by 
building a model of the environment and planning with it.
Compared to model-free methods, they usually take fewer samples to converge to the optimal policy.
Despite that efficiency, model-based methods may not learn the optimal policy due to 
structural modeling assumptions.
In this thesis, we show that by combining model-based methods with 
hierarchically optimal recursive Q-learning under 
a hierarchical reinforcement learning framework, 
the proposed approach learns the optimal policy even when
the assumptions of the model are not satisfied.
%we propose a simple approach that assumes most of the variables 
%are static during the planning process and focuses on modeling the few dynamic variables.

% Embed version information inline -- you should remove this from your
% dissertation
\vfill
\begin{center}
\begin{sf}
\fbox{Revision: \input{version}}
\end{sf}
\end{center}
