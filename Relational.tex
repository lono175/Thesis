
\chapter{Relational Reinforcement Learning}

Video games involves objects: things like monsters, treasures, or princess.
The game world can be described by the relation of objects: there is a
beautiful princess who is prisoned in a castle which is guarded by a horde of
monsters.  When we play the game, it is more nature to consider an action in a
relational form. Are there any monsters near the treasure?  How many monsters
are in the next room?  The states and actions in video games can be more
effectively to be represented in relational forms. 

Reinforcement learning provides a general approach to construct an intelligent
agent with minimal supervision. Nevertheless, most work in reinforcement
learning focuses on propositional representations. Such representation requires
some human expert to construct a fixed-length feature vector to represent the world.
However, it is difficult to construct a fixed-length feature vector to
represent varying number of monsters in video games.
%It is also not clear how to assign each monster to the corresponding feature.
In addition, the
relationships between objects are lost in the propositional representation,
unless it is hand-crafted into the feature vector by the human expert. The lack
of object information makes it hard to transfer the knowledge against one class
of objects to against another similar one. 

The insufficient power of propositional representation motivates the relational
reinforcement learning algorithms.  
The relational reinforcement learning can be formulated as Relational Markov Decision Process (RMDP).


%Dynamic number of objects
%No fixed representation
%Dependency with samples. (intrinsic to reinforcement learning, since consective states has similar
%Q-value, there are not independent all)

%But the most current machine learning algorithms requires the world to be represented 
%by a vector of attributes. The vector representation has several drawbacks when applied to objects.
%The vector representation is ordered. However, it's not clear how to assign a fixed for the objects.
%The length of vector representation is fixed. However, the number of objects might vary.
%The relationship between objects are lost in the vector representation. 

\section{Relational Markov Decision Processes}

Following the definition in \cite{RelationalMDP}, we define 
a relational MDP as follows:

%TODO: Add normal Definition here
\begin{definition} Relational Markov decision process (RMDP) is formalized as a tuple $<\mathbb{P}, \mathbb{A}, \mathbb{D}, T, R>$, where
\begin{itemize}{}
    \item $\mathbb{P}$ is a set of first-order predicates. 
    \item $\mathbb{A}$ is a set of actions.
    \item $\mathbb{D}$ is a domain of constants.
\item A state $S$ in state space $\mathbb{S}$ is a set of all ground literals.
\item The transition function $T:\mathbb{S} \times \mathbb{A} \times \mathbb{S} \rightarrow [0, 1]$ defines a probability distribution over the possible next states. 
\item The reward function $R:\mathbb{S}\times \mathbb{A} \rightarrow \mathbb{R}$ defines the reward after executing a certain action at a certain state.
\end{itemize}
\end{definition}

Assume a RMDP contains $n$ predicates: $\mathbb{P}=\{P_1, \dots, P_n\}$. Each predicate $P_i$ has the form $\bold{Pname}(v_1, \dots, v_k)$.
{\bf Pname} is the name of the predicate, and $v_i$ is a constant. Let $I_{P_i}(S)$ be the set of all positive ground literals
produced by $P_i$ at state $S$. $S$ can be described as $S=I_{P_1}(S)\cup \dots \cup I_{P_n}(S)$. Thus, the predicates in $\mathbb{P}$
provide a complete description of $S$.

%TODO: change it to assumption
%TODO: use Alberta guy's example to motivate it
%TODO: prove it works when certain conditions met (think one dimension example)
%TODO: Show that it's always true (not an assumption) if we have progressive transfer learning. (in the worst case, V(S) = V(everything (the most complex model)))
In this work, we are interested in the value function which has the following property:
\begin{equation}
V(S) = \sum_{L_i \in S} V_{L_i},
\label{eq:relV}
\end{equation}
in which $L_i$ is a positive literal at state $S$. $V_{L_i}$ is the utility which contributes to the whole 
value function when $L_i$ is positive. The above equation indicates that the value function $V(S)$ can be factorized
as the linear combination of the number of positive literals for each predicate. Each positive literal
contributes a constant value to the value function. 


For each step, the agent will receive a new value $V(S_t)$ based on the TD formula:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha [r_{t+1} + \gamma V(S_{t+1}) - V(S_t)],
\end{equation}
However, the utility of each positive literal cannot be observed directly. From equation (\ref{eq:relV}), 
we know the value of $S_t$ is the sum of each utility, therefore we can use linear regression to estimate 
each utility $V_{L_i}$. 

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/monster.eps}
        \caption{A grid world with two monsters (M). A food (F) and an agent (A).}
    \label{fig:Monster}
    \end{minipage}
\end{figure}

Consider the avoid-monster-and-eat-food problem in 
Fig. \ref{fig:Monster}. The world consists of a single agent, a food and a monster.
A reasonable predicate can be $P = {\bf Dist}(O_1, O_2, D)$, which characterizes the
distance between object $O_1$ and $O_2$. The set of literals $I_{P}(S)$ of Fig. \ref{fig:Monster}
is $\{Dist(A, M, 1), Dist(A, M, \sqrt{2}), Dist(A, F, \sqrt{2})\}$.
The domain of constants $\mathbb{D}$ is $\{A, M, F,$ set of some positive real numbers$\}$.
The agent can move in the four possible directions, therefore the action for the agent $\mathbb{A}$ 
is \{North, South, East, West\}.


Though there are many batch learning algorithms available for linear regression, they are not suitable for reinforcement learning
tasks. The batch learning requires a full pass through the data, which not only increases the time
to update the value function as the number of data goes up, but also increases the memory storage. Because 
the value for each time step needs to be stored for the full pass through, it will eventually
exhaust all available memory resources. Thus, online learning is a more appropriate choice
for reinforcement learning. 

There are several Least-Squares Temporal Difference (LSTD) algorithms (\cite{OldLSTD}, \cite{LSTD}, \cite{LSPI} and \cite{TDFA09}) which are proposed for 
learning the value function by basis function approximation for non-relational MDP problems. 
These algorithms conduct linear regression in an online fashion, however, they are not suitable for our 
relational setting. Because we don't have prior knowledge about which relation is relevant to the value function, 
we need to provide a bunch of relations which may be large compared to the samples, and these methods are prone
to over-fitting. 

A approach to avoid over-fitting is to include regularization term in the regression setting. 
Several algorithms for L1 and L2 regularization for TD learning have been proposed (\cite{L1LSTD} and \cite{LassoTD}).
The L1 regularization is particularly suited for our relational setting, since it provide sparse solutions
and can be used as a method for relational template selection.

In this work, we use the approach proposed by Carbonetto et al. \cite{OnlineL1} to estimate the value function.

By estimating the value function by L1-regularized linear regression, we obtain the following relational SARSA algorithm:
\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: relational SARSA\\
\hline
Initialize $\hat{Q_0}$ arbitrarily\\
$t \leftarrow 0$\\
Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $\hat{Q_t}$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a'$ based on $s'$ using policy derived from $\hat{Q_t}$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ $\hat{q_t} \leftarrow \hat{Q_t}(s, a) + \alpha [r + \gamma \hat{Q_t}(s', a')-\hat{Q_t}(s, a)]$ \\
\ \ \ \ \ \ \ \ \ \ \ \ Update $\hat{Q_t}$ with $(s, a, \hat{q_t})$ by online L1 regression to produce $\hat{Q_{t+1}}$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow a'$\\
\ \ \ \ \ \ \ \ \ \ \ \ $t \leftarrow t+1$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}




\section{Progressive Transfer Learning}
To speedup the learning process, it is known \cite{KeepAway} that it is beneficial to allow
the agent to learn from simple scenarios first. In this work, we propose a progressive transfer learning 
approach to gradually transfer the knowledge from simple scenarios to a more complex one.

We start from a simple world which contains one object, and learn the utility associated 
with the predicates with only one object involved. Then we gradually increase the number of 
objects involved in each scenario to train the higher order predicates. 

We denote $V^i$ as the Q-function with the state involved $i$ objects. The objective is to
compute $\Delta{V^i(S)} = V^i(S) - V^{i-1}(S)$. Again, we use the linear approximation to estimate $\Delta{V^i}$:
%TODO: show the state in V^i-1 is obtained by ignoring one object of the state
\begin{equation}
\Delta{V^i}(S) = \sum_{L^i_j \in S} V_{L^i_j},
\label{eq:deltaV}
\end{equation}
where $L^i$ is the $i$th-order predicate. $\Delta{V^i}$ indicates the amount of knowledge we need to 
transfer from a simpler model. It is the residual between the current model and the transferred one.
The objective is to learn the residual function for each training process. If the residual function is 
0 everywhere, we achieve the optimal knowledge transfer.
$V_{L^i}$ is the utility associated with the $i$th-order predicate. We can
see that the utility of a higher order predicate is to estimate what cannot be explained by
lower order predicates. 

The value function $V^i$ can be constructed by:
\begin{equation}
    V^i(S) = V^1(S)+\Delta{V^2(S)}+\dots+\Delta{V^i(S)}
\label{eq:deltaSum}
\end{equation}

Combined with the equation \ref{eq:deltaV}, we can get:
\begin{equation}
    V^i(S) = \sum_{L^1_j \in S} V_{L^1_j}+\sum_{L^2_j \in S} V_{L^2_j}+\dots+\sum_{L^i_j \in S} V_{L^i_j},
\label{eq:deltaSum}
\end{equation}
which is corresponding to our original equation \ref{eq:relV}.

%TODO: our hope is that residual decrease as the order goes up
%if so, we don't need to transfer all the way up. We can just use 2vs 1 to go 10vs9 since the approximation is good enough

%Notice that our objective is to learn the utility associated with each relation.

\section{Experiment}


%TODO: the template in this experiment
%TODO: show the difference of the regressed V and the optimal V for a scenario. It indicates our method
% can estimate the unencountered scenario without sample through it. (work on non-repeating scenario).
%Show the effectiveness of reward explanation
%Show the effectiveness of template selection
%Biased Estimation (if not regression, and slower (require a lot of sampling)
%state it's impossible to do with non-relational approach
We show the effectiveness of our algorithm on a simple avoid-monster-and-eat-food problem, which has been shown
in Fig. \ref{fig:Monster}. The world consists of a single agent, a food and a monster.
The location of all objects are randomly generated at the beginning of each episode.
The agent moves within 8 by 8 grid.
There are 10\% chances for the agent to move in a random direction.
%The size of the state space is $8 \times 8 \times 3 ^ (64 - 1)$
It gets a reward of 20 when it gets a food and -30 if it is caught by
a monster. A penalty term -0.1 would apply for each step. If the agent tries to walk out of the world, it will
stay put and receive -1 reward. With 30\% probability, the monster moves one step toward the location of the agent.
Otherwise, it stays in the original location. 
Each episode ends when the food is consumed or the agent is caught by a monster. 

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/RRL_result.eps}
        \caption{The average reward with and without progressive transfer learning}
    \label{fig:Res}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/Conv.eps}
        \caption{The learning rate of PTL and SARSA agents}
    \label{fig:Res_Learning}
    \end{minipage}
\end{figure}
We compare the result with progressive transfer learning(PTL) and traditional SARSA.
The PTL agent was trained by the simple scenarios with one or two objects. 
The scenario with one object involves the agent itself. The scenarios with two objects involves
the agent and a food or a monster. Each scenario takes one-third of total episodes to train
the agent. The agent needs to transfer its knowledge
from the simple configurations to a more complex testing environment which consists of
three objects with one for each type. The SARSA agent uses all episodes to learn from the complete world, which consists
of three types of objects and is the same as the testing environment. 
Fig. \ref{fig:Res} shows the average reward for 100 episodes. Each agent was trained separately with different number of episodes. 
The figure indicates how the amount of training in the simple scenarios affects the performance of the agent.

Fig. \ref{fig:Res_Learning} shows the learning rate of the PTL and SARSA agents. We trained the PTL agent by 450 episodes.
Then we tested them for 10,000 episodes and average the reward for each 100 episodes. To decrease the time for convergence,
the size of the world is set to 4 by 4 grid and the monster is static.

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/Comp.eps}
        \caption{The average reward for the world consists of 5 food and 2 monsters}
    \label{fig:Res_Complex}
    \end{minipage}
\end{figure}

To show that the efficiency for PTL agent to learn in more complex scenario, we tested the agents in a world with
5 food and 2 monsters. The monster moved toward the location of the agent with 30\% probability. The world is 8 by 8. 
The PTL agent was trained for 600 episodes. We run the experiment with 50,000 episodes.
The result is shown in Fig. \ref{fig:Res_Complex}. 



%The size of the state space is more than $3^{64}$, which is impractical to solve with non-factorized and 
%non-relational approach. We compare our approach with the work in \cite{RRLTD}. We first generated 
%1000 episodes and run the experiments with the episodes to ensure the fair comparison.
%The predicates used in the experiment are listed in the table \ref{tbl:Predicates}.

%The result is not generated yet.

%\begin{table}
%\begin{tabular}[h]{ll}
%\hline
%Features used for the predicate construction\\
%\hline
%$locX(O)$&: the x-coordinate of the object $O$\\
%$locY(O)$&: the y-coordinate of the object $O$\\
%$dispX(O_1, O_2)$&: $locX(O_1) - locX(O_2)$\\
%$dispY(O_1, O_2)$&: $locY(O_1) - locY(O_2)$\\
%$class(O)$&: the class of the object $O$\\
%\hline
%\end{tabular}
%\caption{The features used by predicates.}
%\end{table}

%\begin{table}
%\begin{tabular}[h]{ll}
%\hline
%Predicates used in the experiment\\
%\hline
%$P_{locX}(O, X)$&: true when $locX(O) = X$\\
%$P_{locY}(O, Y)$&: true when $locY(O) = Y$\\
%$P_{dispX}(O_1, O_2, \Delta X)$&: true when $dispX(O_1, O_2) = \Delta X$\\
%$P_{dispY}(O_1, O_2, \Delta Y)$&: true when $dispY(O_1, O_2) = \Delta Y$\\
%$P_{template1}(C_1, C_2, O_1, O_2, \Delta X_2, \Delta Y_2)$&: true when $class(O_1) = C_1$\\
                                                           %&\  and $class(O_2) = C_2$ \\
                                                           %&\  and $P_{dispX}(O_1, O_2, \Delta X_2)$\\
                                                           %&\  and $P_{dispY}(O_1, O_2, \Delta Y_2)$\\
%$P_{template2}(C_1, C_2, C_3, O_1, O_2, \Delta X_2, \Delta Y_2, O_3, \Delta X_3, \Delta Y_3)$&: true when \\
                                                                                             %&\ $P_{template1}(C_1, C_2, O_1, O_2, \Delta X_2, \Delta Y_2)$ \\
                                                                                             %&\ and $P_{template1}(C_1, C_3, O_1, O_3, \Delta X_3, \Delta Y_3)$ \\
%$P_{templateN}(C_1, \dots, C_N, O_1, O_2, \Delta X_2, \Delta Y_2, \dots, O_N, \Delta X_N, \Delta Y_N)$&: true when\\
                                                                                             %&\ $P_{template1}(C_1, C_2, O_1, O_2, \Delta X_2, \Delta Y_2)$ \\
                                                                                             %&\ \ \ \ \ \ \vdots \\
                                                                                             %&\ and $P_{template1}(C_1, C_N, O_1, O_N, \Delta X_N, \Delta Y_N)$ \\
%\hline
%\end{tabular}
%\caption{The predicates for grid world games.}
%\label{tbl:Predicates}
%\end{table}

%We conduct our experiment in $8 \times 8$ grid world. 
%The world consists of an agent, indefinite number of coins and monsters. 
%The location and the number of coins and monsters are decided at the beginning of each episode.
%The goal of the agent is to collect as much coins as possible. Each coin contributes $+10$ reward to the agent.
%After all coins have been collected, the episode ends. If the agent encounters a monster, it will receive
%$-30$ reward and the episode ends immediately. The agent will receive $-1$ reward for each step. 

%Unlike the grid world in previous literature, this game is more challenging because
%the agent faces different scenarios for each episode. It is unlikely that
%an episode will ever repeat itself. The success of the agent lies in how
%to learn from the previous episode and transfer it into the new one.

%The discounted reward factor $\gamma$ is 0.9 and $\alpha$ is 0.2.
%The exploration strategy is $\epsilon-greedy$ with $\epsilon$ equal to 0.2.
%Fig. \ref{fig:AvgReward} shows the average reward per step for the agent.
%Fig. \ref{fig:Policy} shows the learned policy of individual object class after 2,000,000 steps.
%%It indicates that 

%Figure 1: Our agent (S) roams an n by n grid. It gets a payoff of 0.5 for every
%time step it avoids predator (P), and earns a payoff of 1.0 for every piece of food
%(f) it finds. S moves two steps for every step P makes, and P always moves directly
%toward S. When food is found, it reappears at a random location on the next time
%step. On every time step, S has a 10% chance of ignoring its policy and making a
%random move.



%hard to define good concepts "concept" -> feature selection, expert is not reliable
%quote by Eric
% Selection the relevent template of the value function
%enable to differenciate the source of rewards -> regression, they heuristicly sum them up to get the value function
%What is relational template? (capture the local property of a problem and ignore the rest)
%hich applies 
% no prior knowledge about transition and rewards
%model free approach

%TODO: vision perspective-->How to use policy to recognize objects.
%TODO: strength: I found a way to decompose the value function of factoerized MDP into sum of local functions of primitive predicates
% I can change it to sum of composite predicate as well like Relational Temporal Difference Learning by Nima


\section{Rate of Convergence}
Let $V^N = V(O_1, \dots, O_N)$ be the value function of a game which consists of $N$ objects of the same type.
Each object has $K$ states. Therefore, the number of states for $N$ object world is $K^N$.
Our objective is to approximate $V^N$ by:
\begin{equation}
    \hat{V}^N(O_1, \dots, O_N) = \sum_{1 \le i_1 \le N} \Delta V^1(O_{i_1}) + \sum_{1 \le i_1, i_2 \le N} \Delta V^2(O_{i_1}, O_{i_2})
    + \dots + \sum_{1 \le i_1, \dots, i_{N-1} \le N} \Delta V^{N-1}(O_{i_1}, \dots, O_{i_{N-1}})
\end{equation}

\begin{theorem}
Let $R = \max_x |\Delta V^1(x)|$, the magnitude of the largest expected reward in the one object world.
If there exists a constant $a$, $0 < a < 1$, such that $a^iR > \max_x |\Delta V^{i+1}(x)|$ for all $i>1$,
we can achieve $\epsilon$-optimality ($|V^N - \hat{V}^N| < \epsilon$) within total steps $O(MK^{2i^*})$.
\end{theorem}

Proof:\\
Assume we know the transition probability, therefore we can use dynamic programming to solve the MDP problem.
Following the analysis of value iteration in \cite{ComplexityMDP}, we can solve $\Delta V^1 \equiv V^1$ in 
$O(MK^2)$ steps, where $M$ is the number of possible actions. Likewise, $\Delta V^i \equiv V^i - V^{i-1}$ can be computed in $O(MK^{2i})$ steps.
To achieve $\epsilon$-optimality ($|V^N - \hat{V}^N| < \epsilon$), we need to compute the value function up to $i^*$ order, where 
$i^* > \frac{log(\epsilon) - log(R)}{log(a)}$. As a result, the total steps are $O(MK^2) + O(MK^{4}) + \dots + O(MK^{2i^*}) = O(MK^{2i^*})$.
Compared to use dynamic programming to solve the problem directly, it requires $O(MK^{2N})$ steps. When $N \gg i^*$, it is a significant performance boost.

The above analysis assumes the magnitude of the value function decreases a constant factor as the order goes up.
Let's change to a different assumption.

\begin{theorem}
If there are at most $L$ nonzero values for $\Delta V^i$, $i > 1$, we can learn the value 
function with $O(ML^2)$ steps.
\end{theorem}

Proof:\\
To learn $\Delta V^1$ still requires $O(MK^2)$ step. However, $\Delta V^2$ has only $L$ nonzero values. We
can run dynamic programming algorithm specifically to the nonzero locations, which requires $O(ML^2)$.
The overall steps are $O(MK^2) + O(NML^{2})$. When $K^N \gg L$, it can be a huge performance boost as well.

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/EasyExample.eps}
        \caption{A game whose value function can be approximated by first order templates }
    \label{fig:EasyGame}
    \end{minipage}
\end{figure}

Fig. \ref{fig:EasyGame} illustrates a game which satisfies with the assumption of both of the above theorems. 
The agent always starts 
from the leftmost of the grids. The agent receive reward $R$ when it gets a food. The only action of the agent is to go to the right. The discount factor
is $\gamma$. Suppose the food are $F=\{F_1, F_2, \dots, F_N\}$. The value function of a food and an agent
is $\Delta V^1(F_k, A) = \gamma^{d_k}R$, where $d_k$ is the distance between the food and the agent.
The value function for this game is simply $V(F, A)=\sum_k \Delta V^1(F_k, A)$.
The value function of higher order templates are all zeros ($\Delta V^2=\Delta V^3=\dots=\Delta V^N=0$).
Therefore, we can always find a constant $a$ to satisfy $a^iR > \max_x |\Delta V^{i+1}(x)| = 0$ for all $i>1$. 


\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/WallExample.eps}
        \caption{A game whose value function cannot be approximated well by lower order templates}
    \label{fig:WallExample}
    \end{minipage}
\end{figure}
Fig. \ref{fig:WallExample} shows a similar game which includes additional "wall" objects.
This game serves as an example to show when the preconditions of the above theorems cannot be satisfied.
The game consists of an agent, a food and several wall objects. The agent always starts
from the leftmost of the grids, while the food always appear at the rightmost of the grids.
There are several walls between them. The wall blocks the path of the agent, so the agent
cannot get the food after the wall. Assume discount factor for this game is 1.
Let the walls be $W = \{W_1, W_2, \dots, W_N\}$.
$\Delta V^1(F, A) = 1^{d_k}R = R$. Since the agent cannot get any reward from walls, $\Delta V^1(W_k, A) = 0$
However, $V(F, W_k, A) = 0 = \Delta V^1(F, A) + \Delta V^1(W_k, A) + \Delta V^2(F, W_k, A) = R + 0 + \Delta V^2(F, W_k, A)$.
As a result, $\Delta V^2(F, W_k, A) = -R $. Similarly, $V(F, W_k, W_j, A) = 0 = \Delta V^1(F, A) + \Delta V^1(W_k, A) + \Delta V^1(W_j, A) + \Delta V^2(F, W_k, A) + \Delta V^2(F, W_j, A) + \Delta V^3(F, W_k, W_j, A)
= R + 0 + 0 -R -R + \Delta V^3(F, W_k, W_j, A)$. We get $\Delta V^3(F, W_k, W_j, A)=R$. Repeat this process, we get $|V^N(F, W, A)| = R$ for any $N>0$.
It is impossible to find the constant $a$ for this game.

If we let the discount factor to be less than 1, we get $|V^N(F, W, A)| \le \gamma^{N+1}R$. 
It satisfy the conditions of the theorem 1 but does not satisfy conditions of theorem 2.

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/ValueOfOrder.eps}
        \caption{The value range for the first five order templates}
    \label{fig:ValueOrder}
    \end{minipage}
\end{figure}

To evaluate if the magnitude of value function would diminish as the order increases, 
we run the game which is similar to Fig. \ref{fig:EasyGame}, but the agent can be placed to anywhere,
and it can choose to move to left or right.
The result is shown in Fig. \ref{fig:ValueOrder}. It indicates that the magnitude of value drops from 
first order to second order, however, we do not observe the trend continues. 
This example that does not satisfy either of theorem 1 or 2, but it still shows speedup
in our experiment.
Clearly the conditions of theorem 1 and 2 are sufficient but not necessary for the 
speedup of transfer learning.

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/ValueOfOrder.eps}
        \caption{The value range for the first five order templates}
    \label{fig:ValueOrder}
    \end{minipage}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \includegraphics[width=\textwidth] {./figures/TemplateOrder.eps}
        \caption{The comparison of the effectiveness of different order of templates}
    \label{fig:TemplateOrder}
    \end{minipage}
\end{figure}

To show the effectiveness of higher order templates, we run the game with 5 food and 2 monsters
and compare the average reward of them. The result is shown in Fig. \ref{fig:TemplateOrder}.
Each agent has 10,000 training episodes which are evenly distributed among different templates.
Then we run it for additional 20,000 episodes to test. It indicates that under the 
same amount of training, the agents which are trained in complex environments perform 
better than those without. However, the benefits diminish after a certain order.
For example, the agent trained with 2 food performs better than the agent trained with 1 food.
But the agent trained with 3 food performs almost the same as the 2 food one.
In addition, the higher order may not be always better than lower order.
The agent which is trained with 1 food and 2 monster does not perform better than 
1 food and 1 monster. It maybe due to the fact that it's harder for the agent to learn
in 2 monster environment.

\section{Related Work}
The relational reinforcement learning is one of the recent research direction for RL community. 
We provide brief review here. More complete review can be found in \cite{TadSurveyRRL} and \cite{OtterSurveyRRL}.
The relational representation allows us to generalize the existing policy into the complex world, transfer
the knowledge to similar scenarios and handle problems with varying number of objects.
Driessens et al.\cite{TG} combined the Tilde algorithm and the G-algorithm to construct a
relational regression tree to predict Q values. 
%TODO: the approach discovers a huge regression tree for every scenario, which forbids the knowledge transfer
The instance based regression approach \cite{KNN_RRL} computes the weighted average of Q values from the stored
examples. The weights are determined by the distance between the examples.
G\"{a}rtner et al. \cite{GraphKernel} adopted Gaussian processes to approximate Q-functions. Since
Gaussian processes require positive definite covariance function to work, they regards the relations as graphs and
use graph kernels as the covariance function.

There are several works which decompose the value function into several local functions. 
Russel et al.\cite{RessellDecompose} proposed to build a RL agent from simpler subagents. Each subagent has its own
reward function. The value function is the sum of local functions which are learned by each
subagent individually. Guestrin et al.\cite{RelationalMDP} approximates the value function by
linear combination of local functions which are defined for each class of objects.

%TODO: regularized regression approach here
%The approach by Walker et al. (2004) separates the structural induction of the representation
%from the actual value function estimation. First a set of first-order features
%â€“ represented by relational conjunctions over state atoms â€“ is induced. These are then
%used as input for a regression algorithm that estimates Q-value functions per action. The
%method resembles standard function approximation methods for RL (see Section 3.6), using
%first-order features and regularized kernel regression for learning the value function.
%The method resembles standard function approximation methods
%for RL (see Section 2.3.1), using first-order features and regularized kernel regression for
%learning the value function. However, instead of Q-learning, Monte Carlo estimates using
%hand-coded policies are used as training examples.

More recently, Asgharbeygi et al. \cite{RRLTD} proposed relational temporal difference learning 
to learn a distributed value function represented over a conceptual hierarchy of relational predicates. 
Proper et al. \cite{RelationalTemplate} uses relational templates to transfer the knowledge from different 
subdomains of a real-time strategy game. These approaches both approximates the value function by 
the sum of several local functions which are defined over relational predicates or templates. There
are some issues of their approaches. First, they both require an "expert" to define appropriate
predicates or templates. The "expert" may miss some important templates which are necessary for 
the agent to succeed. He may also define some redundant templates so the learning process 
becomes redundant. The second issue is that when an agent receives a reward, they use the reward to update all of the 
local functions. However, the reward may actually be generated by one of the functions, not all of them.
Their approaches are unable to determine the cause of the reward.

In this work, we propose a relational template selection approach to 
select a set of relevant relational templates by L1-regularized regression.
The L1-regularized regression allows us to define a large number of relational
templates and select the relevant templates automatically. Therefore, we do not require
a set of good expert-defined templates. The regression also allows us to explain the cause of the reward.
A reward received by the agent would be explained away by some of the local functions, so it cannot
be claimed by all local functions at the same time.

%Our method resembles standard function approximation methods for RL (Guestrin),
%using first-order features and regression to learn the value function. 
%Nevertheless, in our relational setting, the number of features as well as basis functions are not fixed, but 
%may vary over time.

%Our work is the first who adopts L1 regression on relational reinforcement learning.

%TODO: state the differences with hierarchical approaches

\endinput
%coarse to fine template construction (why it will select coarse template first? because it appears more than complex templates)
%\section{Regression}
%coarse to fine template selection (why it will select coarse template first? because it appears more than complex templates)
%why L1?
%why use online L1?
%how to update the value V by regressed Vhat
%model free
%unknown transition and reward
%find the relevant template


%Another closely related line of work comes from
%Guestrin et al. (2003), who report a system that di-
%rectly approximates the value function by additively
%decomposing it into local functions for each class of
%objects, then calculating weights for the combina-
%tion by solving a linear program using constraint-
%sampling methods. Their approach assumes that re-
%lations among objects do not change over time. Our
%method does not make that assumption, as our learned
%value function is defined over relational predicates
%rather than classes of objects. Less closely related
%Relational Temporal Difference Learning
%approaches include approximate policy iteration (e.g.,
%Fern et al., 2004) and explanation-based reinforcement
%learning (e.g., Boutilier et al., 2001).

%There are many implicit and explicit formalizations of relational versions of MDPs.
%Implicit formalizations (see for example (Guestrin et al., 2003a; Guestrin, 2003; Kersting
%and De Raedt, 2004; Kersting et al., 2004; Fern et al., 2003, 2004; Yoon et al., 2002; Boutilier
%et al., 2001)) use a (fragment of a) first-order language to specify abstract definitions of
%e.g. transition functions and as a consequence, â€“ in an implicit way â€“ an underlying,
%ground relational MDP is specified; i.e. it consists of the semantic level of the language
%used. Explicit formalizations (see for example (Mausam and Weld, 2003; van Otterlo and
%Kersting, 2004; Roncagliolo and Tadepalli, 2004; van Otterlo, 2004)) explicitly define new
%versions of MDPs where the states and actions are defined using ground relational atoms;
%a first-order language is then typically used for abstraction over this MDP definition. Both
%types of formalizations are clearly interchangeable and the main important aspect is the
%relation between the logical abstraction language and the underlying MDP it models. In
%this paper we use a simple definition of an RMDP (van Otterlo, 2004):


%In model-free relational RL, one has studied different
%relational learners for function approximation
%(DÂ·zeroski et al., 2001; Lecoeuche, 2001; Driessens &
%Ramon, 2003; GÃ„artner et al., 2003). Others have applied
%Q-learning based on pre-speciÂ¯ed abstract state
%spaces: Kersting and De Raedt (2003) investigate pure
%Q-learning, Van Otterlo (2004) learns the Q-function
%via learning the underlying transition model. Fern
%et al. (2003) extended previous work on upgrading
%learned policies for small relational MDPS (RMDPs)
%with approximated policy iteration. Finally, Guestrin
%et al. (2003) recently reported on class-based, approximate
%value functions for RMDPs.

%By contrast, previous work on relational reinforce-
%ment learning has focused on different responses to
%the above challenges. Dzeroski et al. (2001) adapted
%relational regression algorithms to take advantage of
%relational structure in describing Q values. For ex-
%ample, the TG algorithm (Driessens et al., 2001) in-
%duces a relational regression tree that predicts Q val-
%ues. However, our system relies on state values so
%that the learning algorithm makes explicit use of the
%domain description.
%Another closely related line of work comes from
%Guestrin et al. (2003), who report a system that di-
%rectly approximates the value function by additively
%decomposing it into local functions for each class of
%objects, then calculating weights for the combina-
%tion by solving a linear program using constraint-
%sampling methods. Their approach assumes that re-
%lations among objects do not change over time. Our
%method does not make that assumption, as our learned
%value function is defined over relational predicates
%rather than classes of objects. Less closely related
%Relational Temporal Difference Learning
%approaches include approximate policy iteration (e.g.,
%Fern et al., 2004) and explanation-based reinforcement
%learning (e.g., Boutilier et al., 2001).


%Through the use of relational regressioní¯€í°€ the RRL sys
%tem 	Dzeroski et alí¯€í°€ 
 %allows the application of
%almost standard Qlearning to reinforcement learning
%problems in environments that are characterized by
%their relational nature
%The use of relational representations of states and
%actions combined with relational regression for Q
%function generalization allows the use of structural in
%formation such as the existence of objects with the
%right properties or relations between objects in the de
%scription of the Qvaluesí¯€í°€ and as a consequence in the
%description of the derived policy This enables the re
%use of experience on smaller but related problems when
%confronted with more elaborate or simply larger tasks
%Three regression algorithms have been developed for
%use in this RRL system the TG algorithmí¯€í°€ which in
%crementally builds rst order regression treesí¯€í°€ an in
%stance based algorithm called RIB and a kernel based
%algorithm KBR that uses Gaussian processes as the
%regression technique
%The TG algorithm 	Driessens et alí¯€í°€ 
 %is a combi
%nation of the Tilde algorithm 	Blockeel  De Raedtí¯€í°€
%
 %that builds rst order classication and regres
%sion trees and the Galgorithm 	Chapman  Kael
%blingí¯€í°€ 
 %that uses a number of statistical values
%concerning the performance of each possible extension
%in each leaf of the tree to build the tree incrementally
%The relational regression trees used by the TG algo
%rithm use conjunctions of rst order literals as tests in
%the internal leafs The test corresponding to a certain
%leaf is the conjunction of the tests appearing on the
%path from the root of the tree to the leafí¯€í°€ in which any
%appearing variable is existentially quantied The TG
%algorithm employs a userdened renement operator
%that originated in the Tilde system to generate the
%possible rst order tests that can be used to replace
%a leaf The statistics stored by the TG algorithm al
%gorithm in each leaf of the tree consist of the number
%of examples classied positively or negatively by each
%possible test and the sum of the Qvalues and squared
%Qvalues in each of these cases This allows the use
%of an Ftest to decide which test to select For nowí¯€í°€
%no tree restructuring is done by TG All decisions that
%the algorithm makes are nal
%The instance based algorithm RIB 	Driessens  Ra
%moní¯€í°€ 
 %uses knearestneighbor prediction as the
%regression techniqueí¯€í°€ ieí¯€í°€ it computes a weighted av
%erage of the Qvalues of the examples stored in mem
%ory where the weight is inversely proportional to the
%distance between the examples The distance used
%needs to be able to cope with relational representa
%tions of states and actions and can be either a gen
%eral purpose rst order distance 	Sebagí¯€í°€  Ra
%mon  Bruynoogheí¯€í°€ 
 %or an applicationspecic
%oneí¯€í°€ which can usually be computed more eciently
%Because Qlearning generates a continuous stream of
%learning examplesí¯€í°€ a number of example selection
%methods were developed to reduce both the memory
%and the computational requirements These selection
%criteria are based on those used in IB and IB 	Aha
%et alí¯€í°€ 
 %and look at the inuence of individual
%examples on the overall prediction error
%The third algorithm is called KBR 	Gartner et alí¯€í°€
%a
 %and uses Gaussian processes as the regression
%technique Gaussian processes 	MacKayí¯€í°€ 
 %require
%a positive denite covariance function to be dened
%between the example descriptions Because of the use
%of relational representations in the RRL systemí¯€í°€ ker
%nels for structured data have to be used to fulll this
%task Possible candidates here are the convolution
%kernel 	Hausslerí¯€í°€ 
 %or kernels dened on graphs
    %Gartner et alí¯€í°€ b
% Because Gaussian processes
%are a Bayesian techniqueí¯€í°€ the KBR algorithm oers
%more than just a basic prediction of the Qvalue of a
%new unseen example It can also give an indication of
%the expected accuracy of this estimateí¯€í°€ which in turn
%can be usedí¯€í°€ for exampleí¯€í°€ by the Qlearning algorithm
%to guide exploration
%One of the major problems that reduces the applica
%bility of Qlearning with relational function abstrac
%tion stems from the nature of Qvalues themselvesí¯€í°€ ieí¯€í°€
%their implicit encoding of both the distance to and the
%size of the next reward These can be very hard to
%predict in stochastic and highly chaotic tasks Other
%approaches such as advantage learning or policy itera
%tion seem more appropriate in such cases

%update by Q by local value function or the global one? (which one will converge faster?)
%context independent ??

%Weakness of current approach
%TG uses a big classification tree to learn all policy of the agent in its lifetime, we need a new way to find the policy against local structure, for transfer
%

%Handle dynamic number of objects (what if a class is missing?) what is the regression tree work?
%Learn simple scenario from complex one--
%Peter Stone's soccer need a simple scenario to transfer
%But in video games we cannot do it
%Current approach cannot transfer from complex one to simple one, how can it be done? From two block to three blocks
%Hanlde real value relations
%Implement the current approach
%the system dynamics and rewards

%spatial proximity
%temporal proximity
%structure learning, group small number of objects into bigger one (CVPR 2010, Yu Pen Fei's project)



at the level of a template for a task domain. Given a particular
environment within that domain, it defines a specific MDP
instantiated for that environment. The domain is specified by a schema,
which specifies a set of object classes C = fC1; : : : ;Ccg. Each class
C is also associated with a set of state variables S[C] =
fC:S1; : : : ;C:Skg, which describe the state of an object in
that class. Each state variable C:S has a domain of possible
values Dom[C:S]. We define SC to be the set of possible
states for an object in C, i.e., the possible assignments to the
state variables of C.
For example, our Freecraft domain might
have classes such as Peasant, Footman, Gold;
the class Peasant may have a state variable
Task whose domain is Dom[Peasant:Task] =
fWaiting, Mining, Harvesting, Buildingg, and a state
variable Health whose domain has three values. In this
case, SPeasant would have 4  3 = 12 values, one for each
combination of values for Task and Health.
The schema also specifies a set of links L[C] =
fL1; : : : ;Llg for each class representing links between objects
in the domain. Each link C:L has a range [C:L] = C0.
For example, Peasant objects might be linked to Barrack
objects â€” [Peasant:BuildTarget] = Barrack, and to the
global Gold and Wood resource objects. In a more complex
situation, a link may relate C to many instances of a
class C0, which we denote by [C:L] = fC0g, for example,
[Enemy:My Footmen] = fFootmang indicates that an instance
of the enemy class may be related to many footman instances.
A particular instance of the schema is defined via a
world !, specifying the set of objects of each class; we use
O[!][C] to denote the objects in class C, and O[!] to denote
the total set of objects in !. The world ! also specifies
the links between objects, which we take to be fixed
throughout time. Thus, for each link C:L, and for each
o 2 O[!][C], ! specifies a set of objects o0 2 [C:L], denoted
o:L. For example, in a world containing 2 peasants,
we would have O[!][Peasant] = fPeasant1;Peasant2g;
if Peasant1 is building a barracks, we would have that
Peasant1:BuildTarget = Barrack1.
The dynamics and rewards of an RMDP are also defined
at the schema level. For each class, the schema
specifies an action C:A, which can take on one of several
values Dom[C:A]. For example, Dom[Peasant:A] =
fWait, Mine, Harvest, Buildg. Each class C is also associated
with a transition model PC, which specifies the probability
distribution over the next state of an object o in class
C, given the current state of o, the action taken on o, and the
states and actions of all of the objects linked to o:
PC(S0
C j SC;C:A;SC:L1 ;C:L1:A; : : : ;SC:Ll ;C:Ll:A): (1)
For example, the status of a barrack, Barrack:Status0,
depends on its status in the previous time step, on
the task performed by any peasant that could build it
(Barrack:BuiltBy:Task), on the amount of wood and gold, etc.
The transition model is conditioned on the state of C:Li,
which is, in general, an entire set of objects (e.g., the set of
peasants linked to a barrack). Thus we must now provide
a compact specification of the transition model that can depend
on the state of an unbounded number of variables. We
can deal with this issue using the idea of aggregation [10].
In Freecraft, our model uses the count aggregator ], where
the probability that Barrack:Status transitions from Unbuilt to
Built depends on ][Barrack:BuiltBy:Task = Built], the number
of peasants in Barrack:BuiltBy whose Task is Build.
Finally, we also define rewards at the class level. We assume
for simplicity that rewards are associated only with the
states of individual objects; adding more global dependencies
is possible, but complicates planning significantly. We define
a reward function RC(SC;C:A), which represents the contribution
to the reward of any object in C. For example, we
may have a reward function associated with the Enemy class,
which specifies a reward of 10 if the state of an enemy object
is Dead: REnemy(Enemy:State = Dead) = 10. We assume
that the reward for each object is bounded by Rmax.
Given a world, the RMDP uniquely defines a ground factored
MDP !, whose transition model is specified (as usual)
as a dynamic Bayesian network (DBN) [3]. The random variables
in this factored MDP are the state variables of the individual
objects o:S, for each o 2 O[!][C] and for each
S 2 S[C]. Thus, the state s of the system at a given point in
time is a vector defining the states of the individual objects in
the world. For any subset of variablesX in the model, we define
s[X] to be the part of the instantiation s that corresponds
to the variables X. The ground DBN for the transition dynamics
specifies the dependence of the variables at time t+1
on the variables at time t. The parents of a variable o:S are
the state variables of the objects o0 that are linked to o. In our
example with the two peasants, we might have the random
variables Peasant1:Task, Peasant2:Task, Barrack1:Status,
etc. The parents of the time t + 1 variable Barrack1:Status0
are the time t variables Barrack1:Status0, Peasant1:Task,
Peasant2:Task, Gold1:Amount andWood1:Amount.
The transition model is the same for all instances in the
same class, as in (1). Thus, all of the o:Status variables for
Figure 2: Freecraft tactical domain: (a) Schema; (b) Resulting factored
MDP for a world with 2 footmen and 2 enemies.
barrack objects o share the same conditional probability distribution.
Note, however, that each specific barrack depends
on the particular peasants linked to it. Thus, the actual parents
in the DBN of the status variables for two different barrack
objects can be different.
The reward function is simply the sum of the reward functions
for the individual objects:
R(s; a) = X
C2C
X
o2O[!][C]
R(s[So]; a[o:A]):
Thus, for reward function for the Enemy class described
above, our overall reward function in a given state will be
10 times the number of dead enemies in that state.
It remains to specify the actions in the ground MDP. The
RMDP specifies a set of possible actions for every object in
the world. In a setting where only a single action can be taken
at any time step, the agent must choose both an object to
act on, and which action to perform on that object. Here,
the set of actions in the ground MDP is simply the union
[o2!Dom[o:A]. In a setting where multiple actions can be
performed in parallel (say, in a multiagent setting), it might
be possible to perform an action on every object in the domain
at every step. Here, the set of actions in the ground MDP is a
vector specifying an action for every object: o2!Dom[o:A].
Intermediate cases, allowing degrees of parallelism, are also
possible. For simplicity of presentation, we focus on the multiagent
case, such as Freecraft, where, an action is an assignment
to the action of every unit.
Example 2.1 (Freecraft tactical
Any text after an \endinput is ignored.
You could put scraps here or things in progress.

Objective: Allow computer to play video games
Objective2: perfect modeling
abundance of old games
home robot entertaunnent(kinetics) join the family
Approach:
Input: Screen and Reward function
1. Video Analysis 
2. Control the game by RL algorithms--RL algorithms must be able to be applied to different games successfully
3. Modeling dynamics(the agent needs to explore the game to get enough information)

Comparison to previous work:
1. Nonintrusive gaming(compared to NIPS 2008)
2. Modeling the game
Chanllenge:
1. Real-Time Video Anaylysis
2. A generic RL algorithm which works on different games
Unlike previous work on RL, the objective is not to design a good AI for a specific game to against
human player, the objective is to design a good and generic AI for play different games successfully
But it is not required to be perfect or optimal. AI in video games cannot be perfect, otherwise it 
would be not possible for a human player to beat the game. The opponent is suboptimal in nature.
3. Little prior knowledge on the games. Unlike keep away, it's not possible to design heiracial action
for (Pong). It must be able to play the game from primitive actions or construct the complex actions by itself.
Volleyball
Example: 
  Fireball vs Soccer ball
  In one game, it's necesart to intercept the soccer ball.
  In another game, it's lethal to touch any ball.
4. Huge game states(640X480X30X(256) per seconds), highly redudunet
5. Little training time (the game has 30fps), cant increase that
6. Dynamic number of agents(avoid ball)(different from soccer)(the number of player is dynamics (unlike game theory))

Motivation for reinforcement learning

Articial Intelligence algorithms that can play classic or video games have been studied ex-
tensively. Research in classic games has resulted in Deep Thought for chess [Campbell et al., 2002],
Chinook for checkers [Schaeer et al., 2005], TD-Gammon for backgammon [Tesauro, 1994],
and Polaris for poker [Bowling et al., 2009]. For AI researchers who work on solving vari-
ous games, there is a recurring question that needs to be addressed: why dedicate limited
resources to solving games instead of tackling the real-world problems in which the eld of
Articial Intelligence can contribute to the daily quality of human lives? In other words,
why spend resources on solving chess, if what we need is self-driving cars with near zero
collision rates? Why play checkers, if what we want is an intelligent maid robot that can
cook, vacuum, and wash the dishes?
The motivation for developing AI agents that can play games is threefold. First, games
oer controlled, well-understood, and easily abstracted environments, with well-dened mea-
sures of success and failure. These properties make games suitable platforms for developing
and testing AI algorithms. The methods developed and the knowledge gained from work-
ing in these controlled environments can later be applied to the real-world problems which
are generally messier and harder to measure performances, but still require the same AI
sophistication.
Additionally, games are excellent platforms for showcasing the capabilities of the latest
AI techniques to the public. In 1997, when Deep Blue defeated Garry Kasparov, a great
wave of excitement was generated among regular, non-academic people about Articial
Intelligence. This is because people understand chess, and respect the complexity involved
in playing it well. Back in 1997, the AI community was not able to develop collision-free
autonomous cars or achieve many other longer-term goals of the eld. Showcasing an agent
that mastered chess helped the public understand what the AI community was capable of
at the time.
Finally, with the recent growth of commercial video games into a multibillion-dollar
industry, there is now even more motivation for studying agents that can learn to act intelli-
gently in games [Lucas, 2009, Laird and Lent, 2000]. Non-repetitive, adaptive, interesting,
and in summary intelligent behavior oers a competitive edge for commercial games. As
the game graphics peak at image-like quality, and as the game consoles oer more and more
computational power that can be spent on complex learning algorithms, the importance of
3


Application:
1. desktop (sort the data row??)
2. gaming ( a alternative of in game AI) can act as opponent or friends (human and computer cooperation)( with 2 different computers)
3. game modeling ( convert to another platform)
2. simulatioin env forj robot
4. agent in online gaming (need to go to toilet)
5. General in game AI
6. General in computer sceice (viki, robot soccer simulated)

high level editing
4. game synthesys (chane the protagonist in game, add monsters)

Problem Statement
The objective of this project is to build a software which can play a wide range 
of video games. To achieve this goal, the software shall not possess any game-specific 
information. Besides, the software shall be able to play the games in a non-intrusive approach.
That is, the software shall be able to extract the necessary information 
from the screenshot of the games and control the games from standard input devices such as keyboard.
Since most of video games use graphic display as the primary interface, this requirement allows
the software to be applied to different video games.

Why video games?
Go beyond Chess and robot soccer:
Over the past decade, substantial research has been conducted to teach computer play classic strategy games such
as Deep Thought for chess [Campbell et al., 2002],
Chinook for checkers [Schaeer et al., 2005], TD-Gammon for backgammon [Tesauro, 1994],
go (Silver, Sutton, and Muller 2007).
and Polaris for poker [Bowling et al., 2009].
On the other hand, little research has been made [cite here] to extend the effort to other genres of video games.
The genres of video games include not only classic strategy games, but also action, first-person shooter, role-player, adventure, simulation, etc\ldots
These games introduce the new challendge to AI community
Complex, can have really large, continuous state and
action spaces.

Can we go beyond the classic ones to investigate the more diversifying video games? 

But why do we need to study suce a topic?
Nowadys, it's quite normal for game company to 
Video games can be viewed as a abstract representation of the real world. Often can we find the 
correspondence between such an ariticial world to a real-world problem<robot soccer, viki>.
Video games allow us to attack a real-world problem without tackling uncessary details, while maintaing enough 
level of abstraction.
Video games are well-understood, custimizable environments. They allow us to work test an AI algorithm
and justify if the result is correct. <alberta, Namir>

Another application is the game industry.
Nowadys, the AI engineers usually need to craft the behavior of the AI by hand. 
The process are time-comsuing and it will produce unrealistic charactor behavior.
If we can design 

Educational


Why vision on video games?

The reason behind is based on the reality that most of the video games do not have source codes publicly available.
If a researcher needs to test his RL alogrithm on Super Mario Bros., all he can do is to apply it on (infinity mario),
since it's the only Mario which goes open source. He cannot test his alrogithm on Mario 1 or 2, which are availbel
on binary. If a researher does not have a source code, he cannot extract the game state like the locaiton
of scoobma which are mandatory for any AI alogrithm. 

Using computer vision techniques to extract the information from the game screen is a way to solve this problem,
since most of the video games uses graphics as the primary interface to interact with the player, it contains
the necessary information for the player to play. The use of computer vision allows us to test the algorithm
non-intrusively, without the effort to hack the game engine or reimplement the game.

Besides, the vision allows a more generic way for a computer to play a video game. It creates 
new applications which cannot be done by intrusive approaches.

Nontrusive and generic AI:
Have you ever experience a good game with poor AI.

--------------High level modeling---------------------
Modding: 
Game modding becomes popular in recent years [modding], 
Modding allows users to customize the video games to suite their personal tastes.
The modding usually includes the introduce new content, modification of oringinal ones, remove the unsatisfactory elements.
The process is usually can be done by the support of game developement kit released by the game company.
It can also be done without the support from the game company. The players need to hacking the game engine,
to decode the data format and develope their own developlment kit. 
Eductional part: --> put the education effort to the game without engine support
It enables the possibility of Nonintrusive modding.
Reuse the AI module in other games. Reuse the content from other games

One engine rules all
In previous, video games are built from scratch. A game company first decides the types of the game,
then then developed the game engine and the content of the game. Later, people starts to figure out 
that the game engine and the content of the game can be separated. The content of the game mostly consist of
artistic materials, dialogs and simple scripte, while the game engines handle the most programming part.
To the game engine, the content is nothing but a set of data. It's more efficient to reuse the same enging
to create multiple games. The reconigction creates the game company which specialized in content, while other
focused on the content. I 
The game companies usually .
Is it possible to use one engine to run all video games?
It is a distinct dream, and cannot be done by policital force.
The modeling of video games also allows us to convert abitrary game into
some unified represention such UML. 
With the universial representation,
it creates the possibility to use one game engine, which serve as the interpret of the content, to run all video games.

It is a dream
The modeling of video games also create the possibility to use 
Platform indepent description of a video game:
There are abunadnt of old games which can 
One simulator for all games
mobile platform: iphone, gphone
web appl: play it on line flash

The modeling of the video games allows us to extract the graphics, the dynamics
and the AI in video games. It enables the possibility to reconstruct the game in 
a high level way. 
--------------High level modeling---------------------


allows the users to modify the 

There has been a recent increase in the number of game environments or engines
that allow users to customize their gaming experiences by building and
expanding game behavior. This article describes the use of modifying, or
modding, existing games as a means to learn computer science, mathematics,
physics, and aesthetic principles. We describe two exploratory case studies of
game modding in classroom settings to illustrate skills learned by students as
a result of modding existing games. We also discuss the benefits of learning
computer sciences skills


Entertainment and Educational Robot:
In the future, 
The robots are not only for chores, it will become a part of the family.
It can listen, and speak with people. 
It won't be a machine which can only execute the instruction from people.
It will give feedback to the poeple. Tell poeple the solution.
It will have be emotionally connected with people.
Teach the young generation how to use computers. 
A future that human and robot can work together and play together.
The possbility shall not be limited to physical games, but also video games.
How can a computer learns how to use another computer.

There will be emotional connection with the robots and people. 

It can play with child,
teach people how to use computer, or even play video games. 
production or domestic services





Computer vision 
1. no source code
Modding
Remodoling
Nonintrusive AI (in different computer)
Gaming robot(Teach people use computer family member, play with people, home robot not chores)
2. have source code, but hard to modify
A general way to manupulate program
3. 



Articial Intelligence algorithms that can play classic or video games have been studied ex-
tensively. Research in classic games has resulted in  For AI researchers who work on solving vari-
ous games, there is a recurring question that needs to be addressed: why dedicate limited
resources to solving games instead of tackling the real-world problems in which the eld of
Articial Intelligence can contribute to the daily quality of human lives? In other words,
why spend resources on solving chess, if what we need is self-driving cars with near zero
collision rates? Why play checkers, if what we want is an intelligent maid robot that can
cook, vacuum, and wash the dishes?
The motivation for developing AI agents that can play games is threefold. First, games
oer controlled, well-understood, and easily abstracted environments, with well-dened mea-
sures of success and failure. These properties make games suitable platforms for developing
and testing AI algorithms. The methods developed and the knowledge gained from work-
ing in these controlled environments can later be applied to the real-world problems which
are generally messier and harder to measure performances, but still require the same AI
sophistication.
Finally, with the recent growth of commercial video games into a multibillion-dollar
industry, there is now even more motivation for studying agents that can learn to act intelli-
gently in games [Lucas, 2009, Laird and Lent, 2000]. Non-repetitive, adaptive, interesting,
and in summary intelligent behavior oers a competitive edge for commercial games. As
the game graphics peak at image-like quality, and as the game consoles oer more and more
computational power that can be spent on complex learning algorithms, the importance of
3
Over the past decade substantial research has been
performed on reinforcement learning (RL) for the robotics
and multi-agent systems (MAS) fields. In addition, many
researchers have successfully used RL to teach a computer
how to play classic strategy games such as backgammon
(Tesauro 1995) and go (Silver, Sutton, and Muller 2007).
However, there has been little research in the application of
RL to modern computer games. First person shooter (FPS)
games have common features to the fields of robotics and
MAS, such as agents equipped to sense and act in their
environment, and complex continuous movement spaces.
Therefore, investigating the affects of RL in an FPS
environment is an applicable and interesting area to
research.

%TODO: add hierarchical template here to motivate our sparse result
%To incorporate our prior knowledge that the value function should be simple and depend on small number of variables, 
%we use L1-regularized regression to ensure the sparsity. 

%\[
%\min_w \norm{Aw-b}^2 + \beta \norm{w}_1
%\]

%Let's us consider the example in fig. \ref{fig:MarioWorld}.
%The world in consists of an agent, a coin and two traps. The agent gets +10 reward when he picks a coin and
%receives -10 reward if he encounters a monster. The agent can choose to move up, down, left or right at
%each turn, but with 0.1 probability that he will choose a random action. The relationship between 
%objects can be described by the distance between the them. The set of positive ground literals are
%${Dist(Agent, Trap, 1), Dist(Agent, Trap, 1.4), Dist(Agent, Coin, 1.4)}$. 
%$Dist(O_1, O_2, d)$ is true
%when the distance between object $O_1$ and $O_2$ is $d$ and false otherwise. 
%The value function of the agent can be represented as $V(S) =
%V_{Dist(Agent, Trap, 1)} + V_{Dist(Agent, Trap, 1.4)} + V_{Dist(Agent, Coin, 1.4)}$.
%The decomposition of value function allows us to handle varying number of
%objects by summation over local functions. The class-specific local function
%also allows to share the knowledge from different objects of the same type.
%The agent knows how to handle a new entity if the agent has experience of the
%same type before. In other worlds, the local functions enable knowledge
%transfer with in the same class.

%\[ Diff(o_1, o_2, d) Template_{c, c_1, c_2}(o, o_1, d_1, o_2, d_2):
%Diff(o, o_1, d_1) and Diff(o, o_2, d_2) Agent(a, o, d, o_1, d_1, o_2, d_2):
%Diff(a, o, d and Template(o, o_1, d_1, o_2, d_2) Loc(a, l) -> captures the
%value which is relevent to the location of the agent.  \]

%$I(trap, s) = {(A, T_1), (A, T_2)}, I(food, s) = {(A, F)}$
%$V(S) = w_{trap}(Dist(A, M_1, 1)) + V_{monster}(Dist(A, M_2, 1.4) +
%V_{coin}(Dist(A, C, 1.4))$. The local functions $V_{monster}$ and $V_{coin}$ indicates
%the expected rewards when there is one such object in the world. The
%decomposition of value function allows us to handle varying number of objects
%by summation over local functions. The class-specific local function also
%allows to share the knowledge from different objects of the same type. 
%The agent knows how to handle a new entity if the agent has experience of the same type
%before. In other worlds, the local functions enable knowledge transfer with in the 
%same class.

%The task of relational reinforcement learning is to find an optimal policy for a RMDP with unknown transition
%function and reward function. As in chapter \ref{ch:RL}, our objective is to find a policy $\pi:\mathbb{S} \rightarrow \mathbb{A}$ 
%which maximizes the value function $V^{\pi}(s) = E_{\pi}[\sum^{\infty}_{t=0} \gamma^{i}R(s_t, \pi(s_t))]$, where $\gamma$ is the discount factor.

%\begin{theorem} If the state transition probability can be factorized as $P(S'|S, A) = P(x_1'|x_1, A)P(x_2'|x_2, A)\dots P(x_k'|x_k, A)$
%and $R(S) = R(x_1) + R(x_2) + \dots + R(x_k)$, where $x_i$ is a subset of $S$ and $S = x_1 \cup x_2 \cup \dots \cup x_k$,
%then the value function can be decomposed into $V(S, A) = V_1(x_1, A) + V_2(x_2, A) + \dots + V_k(x_k, A)$.
%\end{theorem}

%\begin{align*}
%Proof:
%V(S, A) =& \sum_{S'}P(S'|S, A)R(S') \\
%=& \sum_{x_1'} \sum_{x_2'} \dots \sum_{x_k'} P(x_1'|x_1, A)P(x_2'|x_2, A)\dots P(x_k'|x_k, A) [R(x_1') + R(x_2') + \dots + R(x_k')] \\
%=& \sum_{x_1'} P(x_1'|x_1, A)R(x_1') \sum_{x_2'} \dots \sum_{x_k'} P(x_2'|x_2, A)\dots P(x_k'|x_k, A) \\
%&+ \sum_{x_2'} P(x_2'|x_2, A)R(x_2') \sum_{x_1'} \sum_{x_3'}\dots \sum_{x_k'} P(x_1'|x_1, A)P(x_3'|x_3, A)\dots P(x_k'|x_k, A) \\
%&+ \dots \\
%&+ \sum_{x_k'} P(x_k'|x_k, A)R(x_k') \sum_{x_1'} \dots \sum_{x_{k-1}'} P(x_1'|x_1, A)\dots P(x_{k-1}'|x_{k-1}, A) \\
%=& \sum_{x_1'} P(x_1'|x_1, A)R(x_1') +\sum_{x_2'} P(x_2'|x_2, A)R(x_2') + \dots + \sum_{x_k'} P(x_k'|x_k, A)R(x_k') \\
%=& V_1(x_1, A) + V_2(x_2, A) + \dots + V_k(x_k, A)
%\end{align*}

%The value function decomposition allows us to estimate the value function without exploring the 
%exponential state space. What we need is to estimate the individual local value function. It can 
%be done by solving the following equations. 

%\begin{align*}
    %V(S_1) &= |x_1(S_1)|\cdot V_1(x_1(S_1)) + |x_2(S_1)|\cdot V_2(x_2(S_1)) + \dots + |x_k(S_1)|\cdot V_k(x_k(S_1)) \\
    %V(S_2) &= |x_1(S_2)|\cdot V_1(x_1(S_2)) + |x_2(S_2)|\cdot V_2(x_2(S_2)) + \dots + |x_k(S_2)|\cdot V_k(x_k(S_2)) \\
    %&\vdots\\
    %V(S_N) &= |x_1(S_N)|\cdot V_1(x_1(S_N)) + |x_2(S_N)|\cdot V_2(x_2(S_N)) + \dots + |x_k(S_N)|\cdot V_k(x_k(S_N)) \\
%\end{align*}

%To solve the RMDP, common approaches involve relational regression and approximate policy iteration.
%Example: Mario Coin Monster world
%The example state, action, 
%The propositional representation {(1, 24, 3), (2, 35, 6)}
%input is a tuple
%The common approach to solve the RMDP problem
%Feature Engineering
%Relational Regression
%Approximate Policy Iteration


%Motivated by \cite{RelationalTemplate} and \cite{RelationalMDP}, we decompose the value function into
%into the summation of several local functions, which are defined over some templates. 
%Each template represents a policy against a certain type of object.

%The world in fig. \ref{fig:MarioWorld} consists of an agent, a
%coin and two monsters.  The agent gets +10 reward when he picks a coin and
%receives -10 reward if he encounters a monster. The relationship between 
%objects can be described by the distance between the them. $Dist(O_1, O_2, d)$ is true
%when the distance between object $O_1$ and $O_2$ is $d$ and false otherwise. 
%The value function of the
%agent can be represented as $V(S) = V_{monster}(Dist(A, M_1, 1)) + V_{monster}(Dist(A, M_2, 1.4) +
%V_{coin}(Dist(A, C, 1.4))$. The local functions $V_{monster}$ and $V_{coin}$ indicates
%the expected rewards when there is one such object in the world. The
%decomposition of value function allows us to handle varying number of objects
%by summation over local functions. The class-specific local function also
%allows to share the knowledge from different objects of the same type. 
%The agent knows how to handle a new entity if the agent has experience of the same type
%before. In other worlds, the local functions enable knowledge transfer with in the 
%same class.

%There are several challenges arised from this problem.
%First, we don't know the true reward function. An agent may receive a reward because
%he picks up a coin, or he moves into the right of a monster (Fig. ?). There are many 
%ways to explain why the agent receives the reward, but we don't know which one is the real cause.
%Second, we approximate the value function by the summation of several local functions. Yet the potential 
%local functions are many. It's important to choose to the correct local functions to approximate the value function.

%Chanllenge:
%1. The reward function estimation: we don't know which object introduce the reward
%2. What is the correct partition of the value function
%3. The complex transistion: An action may create more objects. A wall may make certain objects unaccessible
%4. object based reward function (the number of reward functions changes over time)
%5. The reason why we can use additive Q function
%6. The problem of Additive Q (failed in highly interactive environment) and Additive SARSA (slow convergence, wrong estimation)
%7. The grid world cahnges every episode (need transfer learning)
%8. Unclear relationship (what is above, what is adjacent, what is mid-range attack, what is touch, what is absorb). for touch, father, it's clear
%9. unknown transition function
%10. unknown reward function, see L1LSTD talk
%11. the curse of dimensionality

%To learn the local value function, we 



%\section{Experiment}
%We conduct our experiment in $8 \times 8$ grid world. 
%The world consists of an agent, indefinite number of coins and monsters. 
%The location and the number of coins and monsters are decided at the beginning of each episode.
%The goal of the agent is to collect as much coins as possible. Each coin contributes $+10$ reward to the agent.
%After all coins have been collected, the episode ends. If the agent encounters a monster, it will receive
%$-30$ reward and the episode ends immediately. The agent will receive $-1$ reward for each step. 

%Unlike the grid world in previous literature, this game is more challenging because
%the agent faces different scenarios for each episode. It is unlikely that
%an episode will ever repeat itself. The success of the agent lies in how
%to learn from the previous episode and transfer it into the new one.

%The discounted reward factor $\gamma$ is 0.9 and $\alpha$ is 0.2.
%The exploration strategy is $\epsilon-greedy$ with $\epsilon$ equal to 0.2.
%Fig. \ref{fig:AvgReward} shows the average reward per step for the agent.
%Fig. \ref{fig:Policy} shows the learned policy of individual object class after 2,000,000 steps.
%%It indicates that 

%\begin{figure}[h]
    %\centering
    %\begin{minipage}[t]{0.6\linewidth}
        %\centering
        %\includegraphics[width=\textwidth] {./figures/AvgReward.eps}
        %\caption{The average rewards acquired by relational template Q-Learning}
    %\label{fig:AvgReward}
    %\end{minipage}
%\end{figure}
%\begin{figure}[h]
    %\centering
    %\begin{minipage}[t]{0.48\linewidth}
        %\centering
        %\includegraphics[width=\textwidth] {./figures/PolicyCoin.eps}
    %\end{minipage}
    %\begin{minipage}[t]{0.48\linewidth}
        %\centering
        %\includegraphics[width=\textwidth] {./figures/PolicyMonster.eps}
    %\end{minipage}
    %\begin{minipage}[b]{0.48\linewidth} \centering (a) \end{minipage}
    %\begin{minipage}[b]{0.48\linewidth} \centering (b) \end{minipage}
    %\caption{(a)The learned policy for a coin (b)The learned policy against a monster}
    %\label{fig:Policy}
%\end{figure}




%\section{Value Function Approximation}
%%TODO: use Literal here
%We approximate the value function as the linear combination over the basis set $h_1, h_2, \dots, h_k$, 
%where the scope of each function $h_i$ is defined by the relational features of template $t_i$.
%\[
%V(s) = sum_i w_i sum_\sigma belong I(i, s) h_i(f_{i, 1}(\sigma), \dots, f_{i, m_i}(\sigma))
%\]

%$I(trap, s) = {(A, T_1), (A, T_2)}, I(food, s) = {(A, F)}$
%$V(S) = w_{trap}(Dist(A, M_1, 1)) + V_{monster}(Dist(A, M_2, 1.4) +
%V_{coin}(Dist(A, C, 1.4))$. The local functions $V_{monster}$ and $V_{coin}$ indicates
%the expected rewards when there is one such object in the world. The
%decomposition of value function allows us to handle varying number of objects
%by summation over local functions. The class-specific local function also
%allows to share the knowledge from different objects of the same type. 
%The agent knows how to handle a new entity if the agent has experience of the same type
%before. In other worlds, the local functions enable knowledge transfer with in the 
%same class.

%\section{Relational Templates}

%The relational templates contain small number of objects of a state. 
%represent part of the states and they occur frequently and are
%relevant to the value function.

%\[
%Diff(o_1, o_2, d)
%Template_{c, c_1, c_2}(o, o_1, d_1, o_2, d_2): Diff(o, o_1, d_1) and Diff(o, o_2, d_2)
%Agent(a, o, d, o_1, d_1, o_2, d_2): Diff(a, o, d and Template(o, o_1, d_1, o_2, d_2)
%Loc(a, l) -> captures the value which is relevent to the location of the agent.
%\]

Beyond Soccer and chess. A new oppourtunituy.

Input of the program:
User cases:
1. Q-Learning
2. SARSA
3. MinMax Q
3. different parameter
4. different game (Pong, Robot Soccer, and Other)
5. different look ahead level of MinMaxQ in different games
6. Cite Sutton work
7. Cite Kevin work
8. Performance comparison of heuristic search vs RL
9. Make MinMaxQ work

Restriction of previous RL:
1. The incapability to generalize (think of an unvisited state)
2. Small number of states
3. fixed represention?

Why RL doesn't work:
dimension
example:
A MDP example with random object for each episode
(sol: Using pairwise Q function to model the Q value between agent and the object, the Q value is centered at the object and we sum it up to get the final Q function for the player)

Future:
A robot can understand how to use computer program in HCI approach


RBM:
action recognition and RL is the same: the diff is that the label of RL is an agent's action
1. heiracial can handle both space and time heiracy, and form the composition actions from the raw data
(solve 66 frames problem)
2. Can use the RBM result to reconstruct the game (how?) 
3. How to deal with dummy agent(action indepent of the env) with real agent(action depends on the env)
4. How to reuse the policy against a certain type of agent to a new unknown agent?
5. How to put the action of an agent into the hierachy? Using supervise human player video is possible, how about unsupervised?
If I can make it work, then the jitter problem of an user agent can be solved(action performed in a reduced dim, no small scale action problem anymore)
How to form a hierachical action policy?
Can use a supervised soccer video to train an agent? therefore the agent can learn the complex action automatically. (keep away soccer)
use a prior to penalize the jitter of an agent's action



6. order the objects from near to far and include the objects action in previous frame



7. the relationship between SOM
8. use different level to quantize the location of the ball and the opponent(use the past 100 frame to predict the future)
9. use different level to model the action of the players(intuition: the dim of the optimal policy is very small)
10. Question: how to create hierachical policy?
11. Iuition: the RL should be both scale and time invariant (if the movement is 0.001 pixel per frame, it should also work)
12. Model the strategy of opponent into macro action, and use them to defeat the opponent. (most video games has built-in AI, can just learn from that, æ…•å®¹å¾©?)
If no opponent, then we have to rely on human.
13. treat action as missing data
14. intuition: the ball movement and the enemy movement are deterministic in most of the games. Therefore the dimension is pretty low.
15. For high frame rate game (like 120 FPS), it's necessary to model the opponent behavior to get the intrinsic action space
otherwise, the random walk is too slow. Or consider the Wolf 3D maze(big maze and small movement). It's too hard to rely on random walk to find the exit.
Each action replies on each expert. The switch action is equal to switch expert. (Hinton's RBM RL)
The qeustion is: when to switch the action? How to handle the big action space problem(can it be small?)
16. Cooperation ML(viki.eecs.harvald.edu)
17. curvature turning is a way to detect the boundary of an macro action (to model the opponent action)(curvature on location(x y) or action(up down left right)?)
It is better to apply it to Andrej's work on virtual avatar walking(human joint movement at walking in 1 dim)
18. create Q value from the actions in the same cluster (same scenario), combine different Q function to build a hierachy(locally trained)
19. Pong->tennis(more strategy, counter attack may fail)
20. Learning from opponent
21. MinMax SMDP Q learning
22. Copy paste programming for video games
23. Genetic algorithm from ada-boosting (you can choose different subset of training data to create different combination 
of actions)
24. collision detection as a prior information is still a way to go (pair-wise relationship modeling)
25. David: Model the transition matrix and immediate reward and use MDP instead.
26. Use convoltition to generate pairwise relationship to multipimpracticalle objects--> A Q function which centered at a block which contains 
3 objects. (spatial hierachy)
27. Fictitous gameplay with convolution. If I have pairwise model, I can simulated the game by put the player in a 
small block which contains 3 or five objects and simulated the real Q function using pairwise model.
28. The answer of taxi driver problem shall be HMM. It is nothing but first order temporal model. (each temporal
state has different task and therefore has different Q function(a function of time))
29. Convert 2D Atari games to 3D automatically (it should not be hard since you have model)(use GameMaker or 3D game studio)
30. Use spatial temporal gassian filter to distribute the rewards. The objects which are near the previous location of mario
will also get high rewards (think spatial and temporal as the same cube)(spatial and temporal proximity)
31. Use internal reward system to teach mario how to jump (if mario move to new Y loc, it gets some rewards)
32. Show the capbility to generalize the object knowledge into different stages
33. Use statistical approach to estimate which object is mario
34. Use statistical approach to estimate which objects are state object, and which objects are real world object (relieve the assumption of my work)
35. Context Free property: Expected rewards are conditional independent for different objects. Can we learn the conditionally independent partial states?
36. How to compose the different policy together?  Can â€œaddâ€ always work? Can we learn how to add?
37. Feature selection for reinforcement learning: Select relevant objects
38. Group lasso, how?
39. Structure Learning to find which variable is dependent on which
( use Mark Schimt approach, set weight to zero, and it remains for zero for all clusters)
Weight 0 means irrelevant. Weight 1 means disjoint, and it is default!!??

40. Group the hierachy based on sptial proximity and temporal proximity, group them toghether.
41. How to learn transition model?
42. Example of Pong in RRL, show that what RRL can do better than RL and Grid World.
43. Generalize the existing policy into unknown objects
44. Discover the same object with different appearance
45. Find the temporal proximity of a configuration.
46. Model-Based vs Model-Free approach
47. Reward distribution problem
48. Learning a policy against certain type of objects from a complex world
49. Find correct cause of the reward
45. Show how to solve MDP with model free and model-based approach. In
model-based appraoch, how to estimate the reward function.  A possible way is
to use Tzu Kuo's Master thesis to do L1 regression when some of the variables
are missing. (sol: just make it 0 for missing variables, it allows the weight
to be anything) For the constant -1 reward which does not depend on anything,
it can be regressed by Y = AX + B + e. B is the constant reward.
You can also do policy iteration by regression trees. Construct the regression tree 
by observing the neareast objects only and gradually expand the tree by adding more objects from near to far.
When there is a missing branch, it means the relation does not exists, which will also provide a value.
It is possible to have multiple branches (more than 2) for a node.

46. Init all unknown state (especiall the location of chicken)to maxV so it will encourge the chicken to walk to the other side (we don't need heirachical RL)
47. the regression approach will learn optimal state (when bullet hits a target). Any action which leads to this state will get high reward
48. RLTD, Gustrin 2003, MaxQ 2003, Russel value decomposition
49. Call this paper: relational template selection and claim novelty and effectiveness against TD-RRL
50. the relationship with Peter Stone's Robot Soccer, experiment on Robot Soccer
51. Experiment on L2 regularization and L1 and no regularization
52. agent location detection
53. threat detection: locate the most threaten objects in screen (it may or may not be nearest one)
54. You can only use VMAX for lower level model (why?)
55. Do you need to seperate different rewards?
56. You need to scale the rewards to [-1, 1], or you need to adjust regularization constant everytime, also, the real weight should fall in [-1, 1]
57. Use forward feature selection from lower order model to higher order and exclude the higher order variable if the lower order model is excluded
58. Use group lasso to select the location of agents.
59. Use different regularization term. regularize higher order predicate heavier? massage the regularization term???
60. Use monte carlo sampling to sample the world in keepaway soccer
61. Show that 3vs2 keepaway to 4vs3 is unneccessary and harmful. You should use 2vs1 to 4vs3. (motivate the necessity to use progressive transfer learning)
62. transfer directly from 2vs 1 to 10vs9
63. Show that without higher order predicate, it's not useful to train in high order scenario, and it can also be harmful
64. Show higher order predicate is useful, but the effect will die away
65. Show progressive learning will outperform any learning in single scenario (1 vs 1 + 2 vs 2 > 2 vs 2), use the same 10 sec criteria
66. Scale up to 9 vs 10
67. The motivation: the real predicates are unknown, you need a way to find true related template. Write it in the perspective of relational templates.
68. Compare with RLTD
69. Relational View of keepaway soccer
70. Show that the common prototype of video games like Raiden or Ys have finite size of relational template(Higher order of templates will vanish)
71. RL is all about finding the shortest path to the reward. I can use model based approach. Build a movement model without object.
    Build a reward model without object. Then put objects one by one, build more complex models. The value function
    can be computed by the weighted shortest path algorithm.































