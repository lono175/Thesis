%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

%TODO: it's possible to learn the opitmal policy with positive pseudo-reward with assumptions on MDP problem

\chapter{Introduction}
\label{ch:intro}

%hard to learn a model: non-smothness of probability and reward function, factoered assumtion, size of planning envelope
%cannot give any optimality guarantees,
Reinforcement learning (RL) addresses the problem of finding an optimal policy in a stochastic environment.
In the RL setting, an agent interacts with the environment, optimizing its behavior to
maximize the received rewards.
RL methods can be broadly classified into two classes: model-based and
model-free. Model-based methods learn an effective policy by
constructing the model from samples and simulating experiences from the model. It
generally requires fewer samples to learn the optimal policy. However, when the
state space is too large, we cannot build the exact model anymore. Instead, we
have to approximate the model with techniques such as function approximation. 
%Little 
%research has been done to address the problem of model-based reinforcement learning with approximation in
%an online setting.
%TODO
Methods based on factored Markov decision processes (FMDPs) assume the problem has some factored structure, and use 
specialized algorithms that exploit the structure \cite{ApproxFactor} \cite{SPUDD}. 
However, these methods require prior knowledge
of the structure of the problem, which may not be available in practice.
Degris and Sigaud \cite{ApproxTree} extended Dyna \cite{Dyna} with the approximation of decision trees.
Sutton et al. \cite{ApproxDyna} introduced linear Dyna -- a combination 
of Dyna and linear function approximation. 
Instead of enumerating all states, which is impossible for large problems, they
use Dyna-style planning. Given the current state and policy, 
their methods predict the features of the next state and reward, using function approximation
techniques. Despite the difficulty of correct prediction, their methods are biased 
in a stochastic environment. It is possible to have
several possible next states given the same state and policy in a stochastic environment.
If we predict the most likely one, we will not find the optimal policy because of the bias in our model.

On the other hand, model-free methods learn the Q-function directly. 
There are no simulation steps for model-free methods and modeling is unnecessary. 
The existing linear function approximation algorithms for model-free methods have been successfully 
applied to large domains \cite{LSTD99}\cite{KeepAway}. 


%The problem is the curse of dimensionality
%supervised learning-> 
%cannot model everything->

In this thesis, we investigate the possibility to combine both model-based and model-free methods 
and get the best of it--simulate the experiences from samples to increase the learning rate
and learn an optimal policy even when the assumption of model is not satisfied. 

We need a framework which can incorporate difference RL algorithms.
One possible choice is 
To combine both methods, we need a framwork 

%In this work, we combine model-based and model-free methods with the hierarchical 
%reinforcement learning (HRL) framework. We propose a simple approximate approach which 
%only enumerates a small number of states during planning, thus it is possible to apply it to large 
%domains. 
%However, the model is too simple to represent complex problems and it may not be able to represent the 
%optimal policy. We show that by combining it with 
%hierarchically optimal recursive Q-learning (HORDQ) \cite{HORDQ}, which is a model-free method, we can 
%guarantee that the overall policy will converge to the optimal one even when our model
%fails to approximate the problem. Our approach assumes the task hierarchy for an MDP is given. The hierarchy can either be 
%designed manually or learned by automatic hierarchy discovery techniques \cite{HexQ}.


%TODO: state that it is fine if we do not learn the transition function or reward function correctly

%\section{Related work}
We are not the first to try to combine different RL methods within the HRL framework.  
Ghavamzadeh and Mahadevan \cite{HybridPolicy} combined value function-based RL and policy gradient RL to handle
continuous MDP problems. Cora \cite{Vlad} incorporated model-based, model-free and Bayesian active learning into the MAXQ framework.
Nevertheless, these methods seek recursive optimality in the learning process, 
thus they fail to satisfy any optimality condition when one of the subtasks
fails to find its optimal policy.
In contrast, our method learns the optimal policy without the requirement that 
all of the policies of the subtasks need be optimal. It is thus more robust and allows us to incorporate approximate
approaches into the same framework.


\section{Contribution}
We are the first to address the problem of learning the optimal policy when some 
suboptimal subtasks are present in the hierarchy.
Our contribution includes: 
\begin{itemize}
\item Derived the condition that the hierarchically optimal policy is equal to the optimal policy when some of the policy of subtasks in the hierarcy are suboptimal
\item Developed a hierarchical reinforcement learning framework which allows unsafe state abstraction without the loss of optimality guarantee
\item TODO: Model based here
\item Introduced the concept of pseudo-reward to HORDQ algorithm and experimentally showed that the existence of pseudo-reward is neccessary for HORDQ to learn faster than the flat Q-learning
\end{itemize}

\section{Thesis organization}
%say the benefit of model-based RL (peter's model based HRL, 
%illustrate the problem of model-based RL

%show that it can be resolved by HRL
%say other HRL work, but no one addresses the optimality with biased model

%At higher level, we use model-based approach to do the planning on a coarser state representation. 
%At lower level, model-free approach is used....
%[Ditterich did that as well]
%Instead of focusing on safe state abstraction, we are more interested in the unsafe one.
%The state of real world problem can be very large and complicated, we may 
%not always find a safe abstracition. In this work, we show that the 
%optimality can still be achieved if 1. 2. 
%[CMU TR][] some uses (unsafe) corasers state at higher level, but none of them 
%provide any guarrante the optimality when the coarser representation are
%not safe, 

%None of the previous approaches 

\endinput
There was no clear advantage for either method in the second half of the experiment. We note that, asymptotically, model-free methods are never worse than model-based methods, and are often better because the model does not converge exactly to the true system because of structural modeling assumptions. (The case we treat here—linear models and value functions with one-step TD methods—is a rare case in which asymptotic performance of model-based and model-free methods should be identical.) The benefit of models, and of planning generally, is in rapid adaptation to new problems and situations 
