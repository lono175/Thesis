%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

%TODO: it's possible to learn the opitmal policy with positive pseudo-reward with assumptions on MDP problem

\chapter{Introduction}
\label{ch:intro}

%hard to learn a model: non-smothness of probability and reward function, factoered assumtion, size of planning envelope
%cannot give any optimality guarantees,
Reinforcement learning (RL) addresses the problem of finding an optimal policy in a stochastic environment.
In the RL setting, an agent interacts with the environment, optimizing its behaviour to
maximize the received rewards. In many applications, it is expensive to 
acquire samples from the environment, so it is important to for the agent 
to learn an effective policy in as few samples as possible. 

RL methods can be broadly classified into two classes: model-based and
model-free.
Model-based methods learn an effective policy by
constructing the model from samples and simulating experiences from the model. It
generally requires fewer samples to learn the optimal policy.

R-MAX \cite{RMaxQ} learns the model by exploring parts of the domain.
To learn an accurate model, the agent needs to explore every state $m$ times.
Thus, it is impractical to apply such method to large domains.

%In R-max, the agent always maintains a
%complete, but possibly inaccurate model of its environment and acts based on the optimal
%policy derived from this model. The model is initialized in an optimistic fashion: all actions
%in all states return the maximal possible reward (hence the name).

For large domains, it is necessary to apply machine learning algorithms to generalize
the knowledge to unvisited states. Learning the model is a supervised learning problem:
given the agent's current state and action, the learning algorithm needs to predict
the next state and reward. Many supervised learning algorithms can generalize 
over unvisited states. 
%Therefore  


Methods based on factored Markov decision processes (FMDPs) assume the problem has some factored structure, and use 
specialized algorithms that exploit the structure \cite{ApproxFactor, SPUDD, Wynkoop08, Walsh09}. 
However, not all of problems have the factored structures. Thus, the applicability of these
methods are limited.

Hester and Stone \cite{Hester09} observed that modeling relative transition effects of actions
is more generalizable than modeling their absolute values. They proposed RL-DT to model relative
effects with decision trees.

Walsh et al. \cite{Walsh09} proposed KWIK (Know What It Knows) linear regression
to learn the transition probabilities and applied it to Stochastic STRIPS domains while
assuming the preconditions and effects of each action are known in advance. 

%OOMDP problem (relational)
%How to learn all effects

%STRIPS domains [8] are made up of a set of objects
%O, a set of predicates P, and a set of actions A. The
%actions have conjunctive (over P) preconditions and
%effects specified by ADD and DELETE (DEL) lists,
%which specify what predicates are added and deleted
%from the world state when the action occurs. Stochastic
%STRIPS operators generalize this representation


%In relational domain, the environment is composed of objects.

%Walsh et al. proposed KWIK linear regression to learn the 
%transition function and 
%the probabilities of action
%outcomes in Stochastic STRIPS and Object
%Oriented MDPs, none of which have been
%proven to be efficiently learnable in the RL
%setting before.




%One way to learn the model of the domain quickly is to 
%introduce generalizationintothelearningofthemodel using 
%modern machine learning techniques. Learning the model is 
%essentially a supervised learning problem where the input 
%is the agents current state and action, and the learning alg
%orithm has to predict the agents next state and reward. 
%Markov Decision ProcessesMany existing supervised learning algorithms are able to 
%generalize their predictions to new or unseen parts of the 
%state space. By applying these methods, the algorithm can 
%learn a model of the domain with far fewer state visits than 
%an algorithm like r-max. 

%Unlike many traditional supervised learning problems, in 
%reinforcement learning, the algorithm has control of which 
%training examples it receives. Learning the model efficiently 
%requires a combination of a fast learning algorithm and a 
%policythat acquiresthenecessary training examplesquickly. 
%The agent can target states for exploration that it expects 
%will improve the model. These states could be where the 
%agent has not visited frequently or where the model has low 
%confidence in its predictions. 

%In R-max, the agent always maintains a
%complete, but possibly inaccurate model of its environment and acts based on the optimal
%policy derived from this model. The model is initialized in an optimistic fashion: all actions
%in all states return the maximal possible reward (hence the name).



%Twoofthemainapproachestowardsthis goal aretoincorpor
%ate generalization(function approximation)intomodel-free 
%methods and to develop model-based algorithms. Model-
%based methods achieve high sample efficiency by learning 
%a model of the domain and simulating experiences in their 
%model, thus saving precious samples in the real world. 

%Once a model-based reinforcement learning method has 
%built an accurate model of the domain, it can quickly find 
%an optimal policy by performing value iteration within its 
%model. Thus the key to making model-based methods more 
%sample efficient is to make their model learning more effic
%ient. 

%Methods such as r-max 
%[2] attempt to learn the modeleffi
%ciently by driving the agenttoexplorepartsofthedomain 
%where the model needs improvement. Still, r-max 
%requires 
%that the agent exhaustively explore every state m times in 
%ordertofullylearn an accuratemodelofthedomain. Particu
%larly in large domains, this exploration can be impractical. 

%One way to learn the model of the domain quickly is to 
%introduce generalizationintothelearningofthemodel using 
%modern machine learning techniques. Learning the model is 
%essentially a supervised learning problem where the input 
%is the agents current state and action, and the learning alg
%orithm has to predict the agents next state and reward. 
%Many existing supervised learning algorithms are able to 
%generalize their predictions to new or unseen parts of the 
%state space. By applying these methods, the algorithm can 
%learn a model of the domain with far fewer state visits than 
%an algorithm like r-max. 

%Unlike many traditional supervised learning problems, in 
%reinforcement learning, the algorithm has control of which 
%training examples it receives. Learning the model efficiently 
%requires a combination of a fast learning algorithm and a 
%policythat acquiresthenecessary training examplesquickly. 
%The agent can target states for exploration that it expects 
%will improve the model. These states could be where the 
%agent has not visited frequently or where the model has low 
%confidence in its predictions. 

%An interesting thought experiment is to consider how a 
%human might behave if put into a gridworld environment 
%with unknown actions and unknown goals. One might try 
%each of the actions a few times to determine what they do, 
%and then assume they will behave roughly the same across 
%the remaining states. Then the person may explore all the 
%states of the domain exhaustively searching for rewarding 
%states. At some point, the human will decide that she has 
%done enough exploration and exploit what she has learned. 

%Little 
%research has been done to address the problem of model-based reinforcement learning with approximation in
%an online setting.
%TODO

%TODO: model-based
%DBN

%Dyna
Degris and Sigaud \cite{ApproxTree} proposed SDYNA, which extended Dyna \cite{Dyna} by
learning the structure of a problem with incremental decision trees.
Sutton et al. \cite{ApproxDyna} introduced linear Dyna -- a combination 
of Dyna and linear function approximation. 
Instead of enumerating all states, which is not feasible for large problems,
their methods predict the features of the next state and reward, using function approximation
techniques. 

%For large domains, it is necessary to apply machine learning algorithms to generalize
%the knowledge to unvisited states. Learning the model is a supervised learning problem:
%given the agent's current state and action, the learning algorithm needs to predict
%the next state and reward. Many supervised learning algorithms can generalize 
%over unvisited states. 

Model-based methods are powerful techniques. 
That allows us to predict the outcome 
of the agent behaviour and plan over it. They can effectively reduce the number of 
samples which are required to find a good policy.
With the learned model, it is easy for model-based methods to generalize
the knowledge to novel scenarios.
However, it is difficult for model-based methods to learn optimal policies.
The possible reasons are:
\begin{itemize}
\item Inaccuracy of the underlying supervised learning algorithms: most of model-based methods rely on supervised learning algorithms
    to learn the model. When these algorithms predict incorrectly, a suboptimal policy might be learned.
\item Structure assumption of the model: most model-based methods have some assumptions on the structure of model.
    For example, the methods based on FMDPs assume the problem has some factored structure; Linear Dyna \cite{ApproxDyna} assumes
    the features of next state can be predicted with linear function approximation. When the assumptions are incorrect, a suboptimal policy will be learned.
\item Impossibility to learn all the effects: as indicated in \cite{Walsh09}, to learn the effects in stochastic STRIPS domains is NP-Hard.
    If the number of all possible effects is small, it is possible to enumerate all of them and exclude the effects with
    small probability. However, since the problem is NP-Hard, there are no known efficient solutions to resolve it.
\item Computational constraints: even if we can learn all the effects, it may be too expensive to consider all of them during
    the planning process. Due to the stochastic nature of MDP, each effect may result in several possible outcomes given
    the same state and action. If we consider too many effects during the planning process, the number of states in a planning envelope
    might become too large to compute.
\end{itemize}


%Despite the difficulty of correct prediction, their methods are biased 
%in a stochastic environment. It is possible to have
%several possible next states given the same state and policy in a stochastic environment.
%If we predict the most likely one, we will not find the optimal policy because of the bias in our model.

%deterministic

%TODO: model-free
On the other hand, model-free methods learn the Q-function directly. 
There are no simulation steps for model-free methods and modeling is unnecessary. 
The existing linear function approximation algorithms for model-free methods have been successfully 
applied to large domains \cite{LSTD99, KeepAway}. 
However, these methods may have slower learning rates since they cannot predict
the outcome of the agent's behaviour and are unable to use planning techniques to 
search for a better policy. Instead, they need to collect the samples by trial-and-error.
This makes it more difficult for these methods to generalize to novel scenarios.
%However, when the
%state space is too large, we cannot build the exact model anymore. Instead, we
%have to approximate the model with techniques such as function approximation. 

%The problem is the curse of dimensionality
%supervised learning-> 
%cannot model everything->

%Model-Based:
%1. Faster learning rate
%2. Possibly worse policy because of structural modeling assumptions
%3. Rapid adaptation to new problems

In this thesis, we investigate the possibility of combining both model-based and model-free methods 
and get the best of each - simulate the experiences from samples to increase the learning rate
and learn an optimal policy even when the assumption of model is not satisfied. 

%Bruno Optimality is not exactly the best target right? Many people are only worried about getting solutions that are good enough. What sacrifices are you making for optimality?
%James The best case is to learn the optimal policy in the minimal steps. 
%James About sacrifice, possibly the learning rate (see experiment of Bus domain), but it is also possible not to sacrifice anything (see Mario experiment)

To combine these two methods, we need a framework which can incorporate different RL algorithms.
One possible choice is hierarchical reinforcement learning (HRL). 
We show that by combining model-based methods with 
hierarchically optimal recursive Q-learning (HORDQ) \cite{HORDQ}, which is a model-free method, we can 
guarantee that the overall policy will converge to the optimal one even when our model
fails to approximate the problem. 
Our approach assumes the task hierarchy for an MDP is given. The hierarchy can either be 
designed manually or learned by automatic hierarchy discovery techniques \cite{HexQ}.


%Bruno How will you guarantee?
%James Please see Theorem 3

%hierarchically optimal recursive Q-learning (HORDQ) \cite{HORDQ}, which is a model-free method, we can 
%guarantee that the overall policy will converge to the optimal one even when our model
%fails to approximate the problem. Our approach assumes the task hierarchy for an MDP is given. The hierarchy can either be 
%designed manually or learned by automatic hierarchy discovery techniques \cite{HexQ}.

%hierarchically optimal recursive Q-learning (HORDQ) \cite{HORDQ}, which is a model-free method, we can 
%guarantee that the overall policy will converge to the optimal one even when our model
%fails to approximate the problem. Our approach assumes the task hierarchy for an MDP is given. The hierarchy can either be 
%designed manually or learned by automatic hierarchy discovery techniques \cite{HexQ}.thods, we need a framwork 

%In this work, we combine model-based and model-free methods with the hierarchical 
%reinforcement learning (HRL) framework. We propose a simple approximate approach which 
%only enumerates a small number of states during planning, thus it is possible to apply it to large 
%domains. 
%However, the model is too simple to represent complex problems and it may not be able to represent the 
%optimal policy. We show that by combining it with 
%hierarchically optimal recursive Q-learning (HORDQ) \cite{HORDQ}, which is a model-free method, we can 
%guarantee that the overall policy will converge to the optimal one even when our model
%fails to approximate the problem. Our approach assumes the task hierarchy for an MDP is given. The hierarchy can either be 
%designed manually or learned by automatic hierarchy discovery techniques \cite{HexQ}.


%TODO: the source of suboptimality (frame assumption and more) the relation between model-based approach and classical planning
%TODO: state that it is fine if we do not learn the transition function or reward function correctly

%\section{Related work}
%Model-based + HRL
%Model-based + Model-free + HRL
We are not the first to try to combine different RL methods within the HRL framework. Ghavamzadeh and Mahadevan \cite{HybridPolicy} combined value function-based RL and policy gradient RL to handle
continuous MDP problems. Cora \cite{Vlad} incorporated model-based, model-free and Bayesian active learning into the MAXQ framework.
Nevertheless, these methods seek recursive optimality in the learning process, 
thus they fail to satisfy any optimality condition when one of the subtasks
fails to find its optimal policy.
In contrast, our method learns the optimal policy without the requirement that 
all of the policies of the subtasks need to be optimal. It is more robust and allows us to incorporate approximate
approaches into the same framework.


\section{Contributions}
%We are the first to address the problem of learning the optimal policy when some 
%suboptimal subtasks are present in the hierarchy.
We are the first to address the problem of learning the optimal policy when
the structural assumption of the model-based methods is not satisfied.
%Our main contribution is to show how to combine model-based and model-free methods
%to increase the learning rate and learn the optimal policy when the assumption of 
%model is not satisfied.

Our contribution are: 
\begin{itemize}
\item Deriving the condition of a task hierarchy that the hierarchically optimal policy is equal to
      the optimal policy when some of the policy of subtasks in the task hierarchy are suboptimal.
\item Developing a hierarchical reinforcement learning framework which allows unsafe state abstraction
      without the loss of optimality guarantee
\item Introducing pseudo-rewards to the HORDQ algorithm and show that it can improve
      the learning rate of the HORDQ algorithm. 
%\item We are the first to have 
\end{itemize}
%\begin{itemize}
%\item Derived the condition that the hierarchically optimal policy is equal to the optimal policy when some policies of subtasks in the hierarcy are suboptimal
%\item Developed a hierarchical reinforcement learning framework which allows unsafe state abstraction without the loss of optimality guarantee
%\item TODO: Model based here
%\item Introduced the concept of pseudo-reward to HORDQ algorithm and experimentally showed that the existence of pseudo-reward is neccessary for HORDQ to learn faster than the flat Q-learning
%\end{itemize}

\section{Thesis organization}
We provide a brief review of reinforcement learning in Chapter 2. 
That includes Markov decision processes (MDPs), semi-Markov decision processes (SMDPs),
and hierarchical reinforcement learning (HRL).
Our main theory is presented in Chapter 3. 
We provide the experimental results on the Bus domain and Infinite Mario in Chapter 4.
Finally, conclusions, limitations, and directions for future work are discussed in Chapter 5.


%say the benefit of model-based RL (peter's model based HRL, 
%illustrate the problem of model-based RL

%show that it can be resolved by HRL
%say other HRL work, but no one addresses the optimality with biased model

%At higher level, we use model-based approach to do the planning on a coarser state representation. 
%At lower level, model-free approach is used....
%[Ditterich did that as well]
%Instead of focusing on safe state abstraction, we are more interested in the unsafe one.
%The state of real world problem can be very large and complicated, we may 
%not always find a safe abstracition. In this work, we show that the 
%optimality can still be achieved if 1. 2. 
%[CMU TR][] some uses (unsafe) corasers state at higher level, but none of them 
%provide any guarrante the optimality when the coarser representation are
%not safe, 

%None of the previous approaches 

\endinput
There was no clear advantage for either method in the second half of the experiment. We note that, asymptotically, model-free methods are never worse than model-based methods, and are often better because the model does not converge exactly to the true system because of structural modeling assumptions. (The case we treat here—linear models and value functions with one-step TD methods—is a rare case in which asymptotic performance of model-based and model-free methods should be identical.) The benefit of models, and of planning generally, is in rapid adaptation to new problems and situations 
