%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Related Work}
\label{ch:Related}

The objective of reinforcement learning algorithm is to build an learning agent. The learning agent will take
actions based on the current state. In the beginning, the agent does not know anything about 
the environment, therefore the agent has to choose the first action randomly. After the environment
receives the action, it will provide a reward to the agent as a feedback. The reward can be either
positive or negative.

The agent adjusts the value function to maximize the expected reward in the future.
The value function represent the expected reward when the agent takes certain action in the current
state, and it is used to estimate the "quality" of an action. The initial value of value function is 
usually 0, but it is possible to set it to some high enough value to encourage exploration.
It is important to evaluate the value function 
correctly. If some actions with low expected reward are estimated as high, it degrades the
performance of agent.

The agent can select an action which leads to the highest value of value function. However, 
this strategy does not allow the agent to explore the states which are not visited before.
A better approach is to use $\epsilon$-greedy method. The method allows the agent to abandon the
best action and choose
a random action with a very small probability $\epsilon$. The higher the probability, the more
likely that the agent would explore the new actions. However, if the exploration probability 
is too high, it will increase the time to converge.

After the agent takes an action, the environment will provide a reward and a state to the
agent. The agent then decides an new action for the new state. After several iterations
, the agent will learn a correspondence between the action and state. The correspondence is called 
"policy". 

\section{Temporal Difference}
\label{sec:TD}
There are 3 types of reinforcement learning algorithms -- dynamic programming(DP), Monte Carlo 
methods, and temporal-difference (TD). Dynamic programming can compute the optimal policy, but it 
requires a precise model of the environment. In most of the cases, the environment
is too complex to be modeled precisely, and it is not easy to get the complete information about
the environment. On the other hand, it is usually possible to use Monte Carlo method to sample the environment to
get the partial information. 
Like Monte Carlo method, TD uses sampling, therefore it does not require the 
complete model of the environment. TD method is a bootstrapping method, similar to the dynamic 
programming approach, it updates the new value function based on the previous one.

The equation to compute the value function in TD:
\begin{displaymath}
   V(S_t) \leftarrow V(S_t) + \alpha [r_{t+1} + \gamma V(S_{t+1}) - V(S_t)],
\end{displaymath}

where $V(s_t)$ is the value function of the state $s_t$. $V(s_t)$ is the expected reward when
the agent reaches the state $s_t$. $r_{t+1}$ is the reward given to the agent when it chooses
the action at state $s_t$.

\section{SARSA}
\label{sec:SARSA}
SARSA is a on-policy TD approach. On-policy indicates that it learns from the current policy.
Different from other TD approaches, SARSA updates the Q value of the current state-action from the next state-action.
The Q value is updated by:
\begin{displaymath}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)],
\end{displaymath}
where $Q(s, a)$ is the value function for state-pair, and it is the expected reward when the agent takes
the action $a$ at the state $s$. $\alpha$ is step-wise, which controls the learning rate. 
$\gamma$ is the discount factor.


\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: SARSA\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a'$ based on $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ \ \ \ \ \ \ $a \leftarrow a'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}

\section{Q-Learning}
\label{sec:Q-Learning}
    Q-Learning is an off-policy TD approach. Compared to SARSA, Q-Learning updates
the Q value by the highest value of the next possible state-action, rather than the 
next state-action executed by the agent.  
The Q value is updated by:
\begin{displaymath}
   Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1}+\gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t)],
\end{displaymath}

where $\max_a Q(s_{t+1},a)$ is the highest value of the next possible state-action. 


\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: Q-Learning\\
\hline
Initialize $Q(s, a)$ arbitrarily\\
Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma max_{a'} Q(s', a')-Q(s, a)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\ \ \ \ \ \ Until $s$ is terminal\\
\hline  
\end{tabular}
\end{center}

\section{minmax Q-Learning}
\label{sec:minmax}

    In two player zero-sum game, it's reasonable to take the action of the opponent into consideration.
In minmax Q-learning, the Q value is a function of state, the action of player, and the action of opponent.
The Q value is updated by:
\begin{displaymath}
    Q(s_t, a_t, o_t) \leftarrow Q(s_t, a_t, o_t) + \alpha [r_{t+1}+\gamma\max_a min_o Q(s_{t+1}, a, o)-Q(s_t, a_t, o_t)],
\end{displaymath}

\begin{center}
\begin{tabular}{@{}lp{6cm}@{}}
\hline
Algorithm: minmax Q-learning\\
\hline
\ \ \ Initialize: $Q(s, a, o) \leftarrow 1, V(s) \leftarrow 1$
\ \ \ Repeat (for each episode)\\
\ \ \ \ \ \ Initialize $s$\\
\ \ \ \ \ \ Repeat (for each step of episode):\\
\ \ \ \ \ \ \ \ \ \ \ \ Choose $a$ based on $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy method)\\
\ \ \ \ \ \ \ \ \ \ \ \ Take action $a$, obtain opponent action $o$, reward $r$ and next state $s'$ from the environment\\
\ \ \ \ \ \ \ \ \ \ \ \ $Q(s, a, o) \leftarrow Q(s, a, o) + \alpha [r + \gamma max_{a'} min_{o'} Q(s', a', o')-Q(s, a, o)]$\\
\ \ \ \ \ \ \ \ \ \ \ \ $s \leftarrow s'$\\
\hline  
\end{tabular}
\end{center}

\endinput
Any text after an \endinput is ignored.
You could put scraps here or things in progress.

Objective: Allow computer to play video games
Objective2: perfect modeling
abundance of old games
home robot entertaunnent(kinetics) join the family
Approach:
Input: Screen and Reward function
1. Video Analysis 
2. Control the game by RL algorithms--RL algorithms must be able to be applied to different games successfully
3. Modeling dynamics(the agent needs to explore the game to get enough information)

Comparison to previous work:
1. Nonintrusive gaming(compared to NIPS 2008)
2. Modeling the game
Chanllenge:
1. Real-Time Video Anaylysis
2. A generic RL algorithm which works on different games
Unlike previous work on RL, the objective is not to design a good AI for a specific game to against
human player, the objective is to design a good and generic AI for play different games successfully
But it is not required to be perfect or optimal. AI in video games cannot be perfect, otherwise it 
would be not possible for a human player to beat the game. The opponent is suboptimal in nature.
3. Little prior knowledge on the games. Unlike keep away, it's not possible to design heiracial action
for (Pong). It must be able to play the game from primitive actions or construct the complex actions by itself.
Volleyball
Example: 
  Fireball vs Soccer ball
  In one game, it's necesart to intercept the soccer ball.
  In another game, it's lethal to touch any ball.
4. Huge game states(640X480X30X(256) per seconds), highly redudunet
5. Little training time (the game has 30fps), cant increase that
6. Dynamic number of agents(avoid ball)(different from soccer)

Motivation for reinforcement learning



Articial Intelligence algorithms that can play classic or video games have been studied ex-
tensively. Research in classic games has resulted in Deep Thought for chess [Campbell et al., 2002],
Chinook for checkers [Schaeer et al., 2005], TD-Gammon for backgammon [Tesauro, 1994],
and Polaris for poker [Bowling et al., 2009]. For AI researchers who work on solving vari-
ous games, there is a recurring question that needs to be addressed: why dedicate limited
resources to solving games instead of tackling the real-world problems in which the eld of
Articial Intelligence can contribute to the daily quality of human lives? In other words,
why spend resources on solving chess, if what we need is self-driving cars with near zero
collision rates? Why play checkers, if what we want is an intelligent maid robot that can
cook, vacuum, and wash the dishes?
The motivation for developing AI agents that can play games is threefold. First, games
oer controlled, well-understood, and easily abstracted environments, with well-dened mea-
sures of success and failure. These properties make games suitable platforms for developing
and testing AI algorithms. The methods developed and the knowledge gained from work-
ing in these controlled environments can later be applied to the real-world problems which
are generally messier and harder to measure performances, but still require the same AI
sophistication.
Additionally, games are excellent platforms for showcasing the capabilities of the latest
AI techniques to the public. In 1997, when Deep Blue defeated Garry Kasparov, a great
wave of excitement was generated among regular, non-academic people about Articial
Intelligence. This is because people understand chess, and respect the complexity involved
in playing it well. Back in 1997, the AI community was not able to develop collision-free
autonomous cars or achieve many other longer-term goals of the eld. Showcasing an agent
that mastered chess helped the public understand what the AI community was capable of
at the time.
Finally, with the recent growth of commercial video games into a multibillion-dollar
industry, there is now even more motivation for studying agents that can learn to act intelli-
gently in games [Lucas, 2009, Laird and Lent, 2000]. Non-repetitive, adaptive, interesting,
and in summary intelligent behavior oers a competitive edge for commercial games. As
the game graphics peak at image-like quality, and as the game consoles oer more and more
computational power that can be spent on complex learning algorithms, the importance of
3


Application:
1. desktop (sort the data row??)
2. gaming ( a alternative of in game AI) can act as opponent or friends (human and computer cooperation)( with 2 different computers)
3. game modeling ( convert to another platform)
22. simulatioin env forj robot
4. agent in online gaming (need to go to toilet)
5. General in game AI
6. General in computer sceice (viki, robot soccer simulated)

high level editing
4. game synthesys (chane the protagonist in game, add monsters)

The objective of this project is to build a program which can play video games 
in a non-intrusive way. The input of the program is the screenshot of a game, a reward,
and the program needs to decide which action to take.
The modeling of the video games allows us to extract the graphics, the dynamics
and the AI in video games. It enables the possibility to reconstruct the game in 
a high level way. 

Problem Statement
The aim of this thesis is to develop AI agents that play Atari 2600 console games. We
are particularly interested in game-independent agents, i.e., agents that are able to play
in at least a large set of Atari 2600 games. As such, no game-specic assumption or prior
knowledge is used in their design or ne-tuning beyond that which is expected to be common
across a diverse set of games. For instance, knowledge that the PC is yellow in a given game
will not generalize to other games and so is not used. However, assumptions that hold true
in a large set of games are permitted. For example, while designing an agent we may assume
that the important game entities have distinct colors.
We consider two types of AI agents: learning based and search based. The learning
based agents use the Sarsa() algorithm and features generated from the game screens and
the console RAM to learn to play an arbitrary game. The search based agents use the
Atari 2600 emulator as a generative model to generate a state-action for any given game.
Full-tree search as well as UCT is applied to this tree with the goal to play as well as
possible by only exploring a small subset of the large state-space. We acknowledge that our
current experiments are limited to one or two specic methods from a large body of possible
approaches, and as such the observations will be limited to these specic methods.


Why video games?
Go beyond Chess and robot soccer:
Over the past decade, substantial research has been made to teach computer play classic games such
as Deep Thought for chess [Campbell et al., 2002],
Chinook for checkers [Schaeer et al., 2005], TD-Gammon for backgammon [Tesauro, 1994],
go (Silver, Sutton, and Muller 2007).
and Polaris for poker [Bowling et al., 2009].
performed on reinforcement learning (RL) for the robotics
There are substantial amount wh
The AI community has been 
Can we go beyond the classic games and robot soccer to investigate the more general, more diverse
video games? 
Articial Intelligence algorithms that can play classic or video games have been studied ex-
tensively. Research in classic games has resulted in  For AI researchers who work on solving vari-
ous games, there is a recurring question that needs to be addressed: why dedicate limited
resources to solving games instead of tackling the real-world problems in which the eld of
Articial Intelligence can contribute to the daily quality of human lives? In other words,
why spend resources on solving chess, if what we need is self-driving cars with near zero
collision rates? Why play checkers, if what we want is an intelligent maid robot that can
cook, vacuum, and wash the dishes?
The motivation for developing AI agents that can play games is threefold. First, games
oer controlled, well-understood, and easily abstracted environments, with well-dened mea-
sures of success and failure. These properties make games suitable platforms for developing
and testing AI algorithms. The methods developed and the knowledge gained from work-
ing in these controlled environments can later be applied to the real-world problems which
are generally messier and harder to measure performances, but still require the same AI
sophistication.
Additionally, games are excellent platforms for showcasing the capabilities of the latest
AI techniques to the public. In 1997, when Deep Blue defeated Garry Kasparov, a great
wave of excitement was generated among regular, non-academic people about Articial
Intelligence. This is because people understand chess, and respect the complexity involved
in playing it well. Back in 1997, the AI community was not able to develop collision-free
autonomous cars or achieve many other longer-term goals of the eld. Showcasing an agent
that mastered chess helped the public understand what the AI community was capable of
at the time.
Finally, with the recent growth of commercial video games into a multibillion-dollar
industry, there is now even more motivation for studying agents that can learn to act intelli-
gently in games [Lucas, 2009, Laird and Lent, 2000]. Non-repetitive, adaptive, interesting,
and in summary intelligent behavior oers a competitive edge for commercial games. As
the game graphics peak at image-like quality, and as the game consoles oer more and more
computational power that can be spent on complex learning algorithms, the importance of
3
Over the past decade substantial research has been
performed on reinforcement learning (RL) for the robotics
and multi-agent systems (MAS) fields. In addition, many
researchers have successfully used RL to teach a computer
how to play classic strategy games such as backgammon
(Tesauro 1995) and go (Silver, Sutton, and Muller 2007).
However, there has been little research in the application of
RL to modern computer games. First person shooter (FPS)
games have common features to the fields of robotics and
MAS, such as agents equipped to sense and act in their
environment, and complex continuous movement spaces.
Therefore, investigating the affects of RL in an FPS
environment is an applicable and interesting area to
research.

Why vision on video games?

1. no source code
Modding
Remodoling
Nonintrusive AI (in different computer)
Gaming robot(Teach people use computer family member, play with people, home robot not chores)
2. have source code, but hard to modify
A general way to manupulate program
3. 



Beyond Soccer and chess. A new oppourtunituy.

Input of the program:
User cases:
1. Q-Learning
2. SARSA
3. MinMax Q
3. different parameter
4. different game (Pong, Robot Soccer, and Other)
5. different look ahead level of MinMaxQ in different games
6. Cite Sutton work
7. Cite Kevin work
8. Performance comparison of heuristic search vs RL
9. Make MinMaxQ work

Restriction of previous RL:
1. The incapability to generalize (think of an unvisited state)
2. Small number of states
3. fixed represention?

Why RL doesn't work:
dimension
example:
A MDP example with random object for each episode
(sol: Using pairwise Q function to model the Q value between agent and the object, the Q value is centered at the object and we sum it up to get the final Q function for the player)

Future:
A robot can understand how to use computer program in HCI approach


RBM:
action recognition and RL is the same: the diff is that the label of RL is an agent's action
1. heiracial can handle both space and time heiracy, and form the composition actions from the raw data
(solve 66 frames problem)
2. Can use the RBM result to reconstruct the game (how?) 
3. How to deal with dummy agent(action indepent of the env) with real agent(action depends on the env)
4. How to reuse the policy against a certain type of agent to a new unknown agent?
5. How to put the action of an agent into the hierachy? Using supervise human player video is possible, how about unsupervised?
If I can make it work, then the jitter problem of an user agent can be solved(action performed in a reduced dim, no small scale action problem anymore)
How to form a hierachical action policy?
Can use a supervised soccer video to train an agent? therefore the agent can learn the complex action automatically. (keep away soccer)
use a prior to penalize the jitter of an agent's action
6. order the objects from near to far and include the objects action in previous frame
7. the relationship between SOM
8. use different level to quantize the location of the ball and the opponent(use the past 100 frame to predict the future)
9. use different level to model the action of the players(intuition: the dim of the optimal policy is very small)
10. Question: how to create hierachical policy?
11. Iuition: the RL should be both scale and time invariant (if the movement is 0.001 pixel per frame, it should also work)
12. Model the strategy of opponent into macro action, and use them to defeat the opponent. (most video games has built-in AI, can just learn from that, 慕容復?)
If no opponent, then we have to rely on human.
13. treat action as missing data
14. intuition: the ball movement and the enemy movement are deterministic in most of the games. Therefore the dimension is pretty low.
15. For high frame rate game (like 120 FPS), it's necessary to model the opponent behavior to get the intrinsic action space
otherwise, the random walk is too slow. Or consider the Wolf 3D maze(big maze and small movement). It's too hard to rely on random walk to find the exit.
Each action replies on each expert. The switch action is equal to switch expert. (Hinton's RBM RL)
The qeustion is: when to switch the action? How to handle the big action space problem(can it be small?)
16. Cooperation ML(viki.eecs.harvald.edu)
17. curvature turning is a way to detect the boundary of an macro action (to model the opponent action)(curvature on location(x y) or action(up down left right)?)
It is better to apply it to Andrej's work on virtual avatar walking(human joint movement at walking in 1 dim)
18. create Q value from the actions in the same cluster (same scenario), combine different Q function to build a hierachy(locally trained)
19. Pong->tennis(more strategy, counter attack may fail)
20. Learning from opponent
21. MinMax SMDP Q learning
22. Copy paste programming for video games
23. Genetic algorithm from ada-boosting (you can choose different subset of training data to create different combination 
of actions)
24. collision detection as a prior information is still a way to go (pair-wise relationship modeling)
25. David: Model the transition matrix and immediate reward and use MDP instead.
26. Use convoltition to generate pairwise relationship to multiple objects--> A Q function which centered at a block which contains 
3 objects. (spatial hierachy)
27. Fictitous gameplay with convolution. If I have pairwise model, I can simulated the game by put the player in a 
small block which contains 3 or five objects and simulated the real Q function using pairwise model.
