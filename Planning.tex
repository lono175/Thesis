%chapter 3:
%1. The power of model based approach--> one wall sample and one pincecess can solve the whole maze problem
   %1.1. show the effectiveness
   %1.2. scaling up--> a noval biased model-based approach (show that it is the only way)
   %1.3. the necessity of biased approach
   %1.4. the choice between planning variable and not
   %1.5. wrong decision variable causes disater (decision)

%chapter 4:
%1. the three optimality ()
%2. Combined with hierarchical optimal RL
%3. why we need leaf cover
%4. the convergence for online approach
%5. the offiline one
%6. Respect the hierarchy

%MAXQ (include the algorithm), hierarchical optimal vs recursive optimal, HORDQ, Relational,  Model-based RL (cite Peter Stone's Model based approach and a brief intro of it, include the algorithm)
%the problem of HO
%Optimal planning, model the differences

\chapter{Planning}

In this work, we follow the same hierarchical formulation as in MAXQ \cite{MaxQJ}:
\begin{definition}
    Given a MDP $M$, the hierarchical reinforcement learning decomposes $M$ into a finite
    set of subtasks $M' = \{M_0, M_1, \dots, M_n\}$, where $M_0$ is the root subtask. 
    Each subtask is defined by 3 tuples $<U_i, A_i, \tilde{R}_i>$. 
    $U_i$ is a termination predicate. It partitions the state space $S$ into active states $S_i$ and
                terminal states $T_i$. If subtask $M_i$ enters any terminal state, it terminates immediately
                and returns control to the parent subtask. 
    $A_i$ is a set of actions which are accessible to subtask $M_i$. An action can be either primitive or composite.
                If it is composite, it simply invokes the corresponding subtask. No recursive calls 
                are allowed in the hierarchy.
    $\tilde{R}_i(s'|s, a)$ is the pseudo reward function. 
\end{definition}
A hierarchical policy $\pi = \{\pi_0, \pi_1, \dots, \pi_n\}$ is a set which contains all subtask policies. 
The subtask policy $\pi_i: S_i \rightarrow A_i$ maps an active state to an action to execute.

%\subsection{MAXQ}
%\subsection{HORDQ}
%Andre and Russell \cite{OptimalQ} extends MaxQ framework to be hierarchical optimal.
%They defined $Q_E(i, s, \pi(s))$ as:
%\begin{equation}
    %Q_E^{\pi}(i, s, \pi(s)) = \Sigma_N \Sigma_{x \in T_i}P(x, N| s, \pi(s)) \gamma^N Q^{\pi}(x, \pi(x)).
    %\label{eq:QE}
%\end{equation}
%$Q_E^{\pi}(i, s, \pi(s))$ is the expected cumulative reward after $i$-th node follows 
%policy $\pi$ at state $s$ and terminates at some point.

%In theorem ?, we show that if we modify the hierarchy and let some subtasks to have access all primitive actions, 
%the hierarchical optimal policy is equal to the optimal policy.

\section{Recursive Optimality and Hierarchical Optimality}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=4in] {./figures/Maze.eps}
\end{center}
\caption{The task hierarchy of the taxi domain \cite{MaxQJ}.}
\label{fig:taxi}
\end{figure}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=4in] {./figures/MazeH.eps}
\end{center}
\caption{The task hierarchy of the taxi domain \cite{MaxQJ}.}
\label{fig:taxi}
\end{figure}

There are three definitions of optimality of hierarchical reinforcement learning:

\begin{definition}
    \textbf{Optimality:} An optimal policy $\pi^*$ for MDP $M$ is a policy that achieves the highest cumulative reward
    among all policies for the MDP.
\end{definition}
\begin{definition}
    \textbf{Hierarchical Optimality:} Given $Pi_H$, which is the set of all policies consistent with hierarchy $H$, 
    then a hierarchical optimal policy for MDP $M$ is a policy $\pi^* \in Pi_H$ that achieves the highest cumulative reward
    among all policies $\pi \in Pi_H$.
\end{definition}
\begin{definition}
    \textbf{Recursive Optimality:} A recursive optimal policy for MDP $M$ with hierarchical 
    decomposition $M' = \{M_0, M_1, \dots, M_n\}$ is a policy $\pi = \{\pi_0, \pi_1, \dots, \pi_n\}$ such
    that each policy $\pi_i$ achieves the highest cumulative pseudo-reward for the corresponding subtask $M_i$,
    assuming fixed policies for its action $A_i$, 
\end{definition}


%Recursive and hierarchical optimality are two impor-
%tant forms of optimality in hierarchical reinforcement
%learning. Recursive optimality only guarantees that
%the policy of each subtask is optimal given the policies
%of its children. It is an important form of optimality
%because it permits each subtask to learn a locally opti-
%mal policy while ignoring the behavior of its ancestors
%in the hierarchy. This increases the opportunities for
%subtask sharing and state abstraction. The original
%MAXQ HRL algorithm (Dietterich, 2000) converges
%to a recursively optimal policy. On the other hand,
%hierarchical optimality is a stronger form of optimal-
%ity since it is a global optimum consistent with the
%given hierarchy. In this form of optimality, the policy
%for each individual subtask is not necessarily optimal,
%but the policy for the entire hierarchy is optimal. The
%HAMQ HRL algorithm (Parr, 1998) and the SMDP
%learning algorithm for a xed set of options (Sutton
%et al., 1999) both converge to a hierarchically optimal
%policy.
%The following example from (Dietterich, 2000) demon-
%strates the dierence between recursively and hierar-
%chically optimal policies. Consider the simple maze
%problem in gure 1. Suppose a robot starts somewhere
%in the left room and it must reach the goal G in the
%right room. In addition to three primitive actions,
%North, South and East, the robot has a high level task
%Exit Room in the left room and a high level task Go to
%Goal in the right room. Exit Room terminates when
%the robot exits the left room and Go to Goal termi-
%nates when the robot reaches the goal G. The arrows
%in gure 1(a) show the locally optimal policy within
%each room. The arrows in the left room seek to exit
%the room by the shortest path. The arrows in the right
%room follow the shortest path to the goal. However,
%the resulting policy is not hierarchically optimal. Fig-
%ure 1(b) shows the hierarchically optimal policy that
%would always exit the left room by the upper door.
%This policy would not be locally optimal because the
%states in the shaded region would not follow the short-
%est path to the doorway.

%TODO: which approach is HO and which is RO
%TODO: MAXQ
%TODO: the problem of RO
%TODO: HORDQ which approach is HO and we choose HORDQ
%TODO: the problem of HO
\section{Optimal Planning with Task Hierarchy}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=4in] {./figures/TaxiHierarchy.eps}
\end{center}
\caption{The task hierarchy of the taxi domain \cite{MaxQJ}.}
\label{fig:taxi}
\end{figure}

%The primary contribution of this work is to show that by combining model-based and 
%model-free approaches, we can still achieve optimality even when the model is biased.

We illustrate our idea with the task hierarchy (Fig. \ref{fig:taxi}) of the taxi domain introduced by Dietterich \cite{MaxQJ}.
The taxi problem is an episodic task. For each episode, the taxi starts
at a random location. To finish the task, the taxi needs to go to the passenger's location, pick up the passenger,
go to the destination, and put down the passenger. The task can be further decomposed two subtasks: $Get$ and $Put$. 
The goal of subtask $Get$ is to move the taxi to the passenger's location and pick up the passenger. The goal of 
subtask $Put$ is to put down the passenger at the destination.

Assume the policy of subtask $Root$ is suboptimal and always invokes $Get$ even when 
the passenger is already in the taxi. Optimality can be guaranteed if subtask $Get$
learns to deliver the passenger to his destination.
Or suppose the policy of $Root$ always chooses $Put$.
If subtask $Put$ learns to pickup the passenger when he is not in the taxi, 
we will have the optimal policy because it does not matter which decision is made 
by $Root$, the passenger can always be picked up and delivered to the destination.

The above example provides two observations.
First, in order to guarantee optimality, subtasks $Get$ and $Put$ need 
to act optimally in regards to the goal of whole problem, not 
the subgoal of each subtask. It implies that we need to seek hierarchical optimality
rather than recursive optimality.

Second, optimality cannot be guaranteed without modification of the hierarchy. In the original hierarchy, 
subtask $Get$ has no access to action $Putdown$. Even though subtask $Get$ delivers the passenger to the destination,
it cannot put him down. We need to modify the hierarchy to let subtask $Get$ be
able to solve the problem on its own. A way to achieve that is to let subtask $Get$ have access
to action $Putdown$. 

%We can only guarantee optimality by adopting HORDQ for certain positions
%in the hierarchy. The "positions" are defined by the following definition:

We define which subtasks in a given hierarchy shall act optimally with the following definition:
\begin{definition}
    $C(H) = \{M_{j_1}, M_{j_2}, \dots, M_{j_k}\}$ is a leaf cover of hierarchy $H$ 
    if there is no subtask $M_i \notin C(H)$ which has access to a primitive action.
    Furthermore, $TC(H)$ is a total leaf cover if it is a leaf cover and all primitive
    actions are directly or indirectly (through child subtasks) accessible for every 
    subtask $M_i \in TC(H)$.
\end{definition}

%A leaf cover contains the subtasks which are able to solve the problem on their own.
We can always find a leaf cover for a hierarchy by including all subtasks which have access
to primitive actions. The total leaf cover can be constructed from the leaf cover by
adding the missing primitive actions.
Consider the task hierarchy in taxi domain in Figure \ref{fig:taxi},
if we add action $Pickup$ and $Putdown$ to subtask $Navigate$, 
we get a total leaf cover which consists of subtasks $Get$, $Put$, and $Navigate$.
%Another way is to let Navigate, Get and Put subtasks
%have access to all six primitive actions and include these subtasks in the total leaf cover. 

This conversion increases the exploration space because each subtask needs to explore more actions. It may increase 
the time to learn the optimal policy. However, as we show in our experiment, a good approximate model can effectively increase
the learning rate, so the time to learn the optimal policy might still decrease overall.

%There 
%It is worth noting that we can get the same optimality guarantee with any 
%hierarchical optimal learning algorithms such as HAMQ\cite{HAMQ} or tracked decomposed Q-learning\cite{HORDQ}. However, HORDQ can be easily integrated with 
%the existing MAXQ hierarchy, while the decomposed representation of Q-function is preserved.

Andre and Russell \cite{OptimalQ}\cite{HORDQ} introduced hierarchical optimal recursive decomposed Bellman equations
which extend the decomposition of MAXQ in a way that hierarchical optimality
can be guaranteed.

The Q-value is decomposed as:
\begin{align}
    \label{eq:HordQ}
    Q^{\pi}(i, s, a) = E[\sum_{t=0}^{\infty}\gamma^t r_t] &= E[\sum_{t=0}^{N_1 - 1}\gamma^t r_t] + E[\sum_{t=N_1}^{N_2 - 1}\gamma^t r_t] + E[\sum_{t=N_2}^{\infty}\gamma^t r_t]\\
                    &= Q_r^{\pi}(i, s, a) + Q_c^{\pi}(i, s, a) + Q_e^{\pi}(i, s, a),
\end{align}
where $r_t$ is the random variable of the reward that the agent receives at step $t$, $N_1$ is the number of primitive actions to finish action $a$, 
and $N_2$ is the number of primitive actions 
to finish subtask $M_i$. $ Q_r^{\pi}$ is the expected cumulative reward for executing action $a$.
$Q_c^{\pi}$ is the expected cumulative reward when subtask $M_i$ finishes after the execution of action $a$. 
$Q_e^{\pi}$ is the expected cumulative reward when the episode ends after the execution of subtask $M_i$ .

%TODO: $Q_r$ 

%Our objective is to prove that we can learn the optimal policy if we use HORDQ \cite{HORDQ} on the subtasks which 
%belong to a total leaf cover.

%The node queries its children node to get the value of $V^{\pi}(a, s)$.
$Q_r^{\pi}$ can be computed as:
\begin{equation}
    Q_r^{\pi}(i, s, a) = 
    \left\{\begin{array}{ll}
        Q_r^{\pi}(a, s, \pi_a(s)) + Q_c^{\pi}(a, s, \pi_a(s))& \mbox{if $a$ is composite} \\
        \Sigma_{s'} P(s'|s, a)R(s'|s, a) & \mbox{if $a$ is primitive} \\  
    \end{array} \right.
    \label{eq:Qr}
\end{equation}
%In our example, to compute $Q^{\pi}(MaxRoot, s, GotoExit)$, "MaxRoot" node would query 
%"MaxExit" node to get $V^{\pi}(GotoExit, s)$.

$Q_c^{\pi}$ can be computed as:
\begin{equation}
    Q_c^{\pi}(i, s, a) = \sum_{s', N} P_{S_i}^{\pi}(i, s', N|s, a)\gamma^N[Q_r^{\pi}(i, s', \pi_i(s')) + Q_c^{\pi}(i, s', \pi(s'))],
    \label{eq:Qc}
\end{equation}
where $P_{S_i}^{\pi}(i, s', N|s, a)$ is the probability that $s'$ is the first state in $S_i$ which
is encountered after the execution of action $a$ which takes exactly $N$ steps to finish. 

%Note that $P(s'|s, \pi(s))$ and $V^{\pi}(a, s)$ are provided by the child Max nodes.
And $Q_e^{\pi}$:
\begin{equation}
    Q_e^{\pi}(i, s, a) = \sum_{s', N} P_{T_i}^{\pi}(k, s', N|s, a)\gamma^N[Q^{\pi}(k, s', \pi_k(s'))],
    \label{eq:Qe}
\end{equation}
where $k$ is the index of parent subtask which invoked subtask $M_i$.

To guarantee the optimality, we cannot use the Q-value of parent subtask $M_k$ to update $Q_e^{\pi}$ of subtask $M_i$ because the 
Q-value might not be correct due to the biased model.  Instead, 
we update $Q_e^{\pi}$ with the Q-value of next subtask in $TC(H)$.
Hence, we modify equation \ref{eq:Qe} as:

\begin{equation}
    Q_e^{\pi}(i, s, a) = \sum_{s', N} P_{T_i}(i', s', N|s, a)\gamma^N[Q^{\pi}(i', s', \pi_{i'}(s')],
    \label{eq:OptQe}
\end{equation}
where $i'$ is the next subtask in $TC(H)$ that will be invoked, $P_{T_i}^{\pi}(i', s', N|s, a)$ is the probability that $s'$ is the first state in $T_i$ which
is encountered after the execution of action $a$ which takes exactly $N$ steps to finish. 
Note that the property of leaf cover ensures that we can always find such subtask $M_{i'}$ before any primitive
action is executed. %TODO: (why?).

\begin{theorem}
    Let $C(H)$ be a leaf cover of hierarchy $H$, if $Q^{\pi} = Q_r^{\pi} + Q_c^{\pi} + Q_e^{\pi}$ and
    $Q_r^{\pi}$, $Q_c^{\pi}$,and $Q_e^{\pi}$ follow equations (\ref{eq:Qr}-\ref{eq:Qc}) and (\ref{eq:OptQe}), we have $Q^{\pi}$ satisfies:
    \begin{equation*}
    Q^{\pi}(i, s, a) = 
    \left\{\begin{array}{ll}
        \sum_{s'}P(s'|s, a)[R(s'|s, a) + \gamma Q^{\pi}(i', s', \pi_{i'}(s'))], &\mbox{if $a$: primitive} \\
        Q^{\pi}(a, s', \pi_{a}(s')), &\mbox{if $a$:composite}
    \end{array} \right.
    \end{equation*}
    \label{thm:Bell}
\end{theorem}
Although we changed the formula for $Q_e^{\pi}$, the argument of Theorem 10 in \cite{HORDQ} still holds.
We do not repeat the proof here. 

%TODO: not all s are defined in Q(i, s, a) (focus on si instead of all i)
%TODO: what if pi_c_bar may change for every step?
%TODO: what if pi_c_bar is not deterministic
%TODO: write down the Q learning algorithm and the model-based one
%TODO: why Q^*(i, s, a) = Q^*(s, a) shows that hierarchical pi is the optimal policy
%TODO: any rigorous property for leaf cover?
%TODO: do I use all assumption for the theorem?
%TODO: the relationship between HORDQ and my approach
%TODO: provide the reason why we need to acceess all primitive action (because of the dumb and never learn planner)
%TODO: show that I can convert any MDP problem to hierarchy one, thus we can always combine approximated model-based approach with HORDQ.


With Theorem \ref{thm:Bell}, we can prove that the optimal policy can be learned if there exists a 
total leaf cover for the hierarchy:
\begin{theorem}
    %$\forall \pi_{N}: A_{TC} \times S \rightarrow A_{TC}$,   
    %TODO: state that the proof does not require
    Given an MDP $M$ and a hierarchy $H$ that decomposes $M$ into a finite set of subtasks $M' = \{M_0, M_1, \dots, M_n\}$,
    let $A_p$ denote the set of primitive actions for $M$, $Q^*(i, s, a)$ be the optimal Q-function for subtask $M_i$, and
    $Q^*(s, a)$ be the optimal Q-function for $M$. If $TC(H)$ is a total leaf cover of $H$,
    we have $Q^*(i, s, a) = Q^*(s, a), \forall s \in S_i, a \in A_p, M_i \in TC(H)$
    \label{thm:opt}
\end{theorem}
\textbf{Proof:} 
    Let $\pi_f: S \rightarrow A_p$ be a policy for $M$. We can construct a hierarchical policy $\pi$, such that
    $\pi_i(s) = \pi_f(s) = a$, if $a \in A_p$, $\forall M_i \in TC(H)$ (if $a$ is not directly accessible by $M_i$, we can
    let $\pi_i(s)$ be one of its composite actions).
    From Theorem \ref{thm:Bell}, we know: 
    \begin{equation}
        Q^{\pi}(i, s, a) = \sum_{s'}P(s'|s, a)[R(s'|s, a) + \gamma Q^{\pi}(i', s', \pi_{i'}(s'))].
    \end{equation}
    If $a_2= \pi_{i'}(s')$ is a composite action, we have $Q^{\pi}(i', s', a_2) = Q^{\pi}(a_2, s', \pi_{a_2}(s'))$. 
    We can keep applying the substitution until $\pi_{a_k}(s') \in A_p$, for some $k$. Since there are no
    indirect or direct recursive calls allowed in the hierarchy, the substitution can be done in finite 
    steps. Now we have:
    \begin{equation}
        Q^{\pi}(i, s, a) = \sum_{s'}P(s'|s, a)[R(s'|s, a) + \gamma Q^{\pi}(a_k, s', \pi_{a_k}(s'))].
        \label{eq:MaxIrr}
    \end{equation}
    Note that $a_k \in TC(H)$ because all subtasks which have direct access to primitive actions belong to $TC(H)$.
    By the construction of $\pi$, we have $\pi_{a_k}(s') = \pi_f(s')$.
    
    Compare (\ref{eq:MaxIrr}) to the Bellman equation of the flat MDP:
    \begin{equation}
        Q^{\pi_f}(s, a) = \sum_{s'}P(s'|s, a)[R(s'| s, a) + \gamma Q^{\pi_f}(s', \pi_f(s'))].
        \label{eq:bellman}
    \end{equation}

    Equations (\ref{eq:MaxIrr}) and (\ref{eq:bellman}) are identical except for the Q values.
    Due to the uniqueness of the Bellman equation, we have $Q^{\pi_f}(s, a) = Q^{\pi}(i, s, a), \forall s \in S_i, a \in A_p, i \in TC(H)$. 
    If $\pi_f(s) = \pi^*_f(s)$, $Q^*(s, a)$ is a solution to equation (\ref{eq:MaxIrr}). So we have $Q^*(i, s, a) \geq Q^*(s, a)$.
    Since a hierarchical policy cannot be better than an optimal policy, we have $Q^*(s, a) \geq Q^*(i, s, a)$.
    Thus we have $Q^*(i, s, a) = Q^*(s, a)$. \textbf{Q.E.D.}

Note that the above proof does not pose any constraints on the policy of subtasks $M_i \notin TC(H)$. 
If we adopt any learning algorithms for such subtasks, we still have the same optimality guarantee. 
We can compute the optimal policy using either policy iteration or value iteration algorithms. The arguments of
Theorem 11 and 13 of \cite{HORDQ} hold in our case. We do not repeat the arguments here.
 
%Because we do not usually have the complete knowledge of reward function and transition function,
%it is more desirable if we can estimate the $Q$ value in an online fashion.
The previous equations assume we have complete knowledge about the problem and can compute
the Q-value with dynamic programming. If not, we can estimate the Q-value with
the hierarchically optimal recursive Q-learning (HORDQ) \footnote{Also called ALispQ-learning in \cite{OptimalQ}} \cite{HORDQ} 
update rules:
%use temporal difference learning instead:
%TODO: the discount term compared the Bellman 
%TODO: the value iteration here!!!

\begin{equation}
    Q_r^{t+1}(i, s, a) =
    %\left\{\begin{array}{ll}
    (1 - \alpha_t)Q_r^t(i, s, a) + \alpha_t R_t(s'| s, a)   \mbox{ if $a$ is primitive} \\
    \label{eq:TdQr}
\end{equation}
%\begin{equation}
    %\tilde{Q}_r^{t+1}(i, s, a) \leftarrow
    %\tilde{Q}_r^{t}(i', s, a') + \tilde{Q}_c^{t}(i', s, a')  \mbox{if $a$ is composite} \\
    %%\end{array} \right.
    %\label{eq:TdQr}
%\end{equation}
\begin{equation}
    Q_c^{t+1}(i, s, a) =
    %\left\{\begin{array}{ll}
    (1 - \alpha_t)Q_c^t(i, s, a) + \alpha_t \gamma^N[Q_r^{t}(i', s', a') + Q_c^t(i', s', a')] \\
    \label{eq:TdQc}
\end{equation}
%\begin{equation}
    %\tilde{Q}_c^{t+1}(i, s, a) \leftarrow
    %(1 - \alpha)\tilde{Q}_c^{t}(i, s, a) + \alpha \gamma^N[\tilde{Q}_r^{t}(i', s', a')]   \mbox{if $s' \in T_i$} \\
    %%\end{array} \right.
    %\label{eq:TdQc}
%\end{equation}

\begin{equation}
    Q_e^{t+1}(i, s, a) =
    \left\{\begin{array}{ll}
    (1 - \alpha_t)Q_e^{t}(i, s, a) + \alpha_t \gamma^N[Q_e^{t}(i', s', a')]  \mbox{ if $s' \in S_i$} \\
    (1 - \alpha_t)Q_e^{t}(i, s, a) + \alpha_t \gamma^N[Q^{t}(i', s', a')]  \mbox{ if $s' \in T_i$} \\
    \end{array} \right.
    \label{eq:TdQe}
\end{equation}
where $i'$ is the index of next subtask in $TC(H)$ that will be invoked, $a' = arg max_b Q^t(i', s', b)$.

Unfortunately, the convergence of HORDQ is an open problem (Conjecture 1 of \cite{HORDQ}).
However, if we let all subtasks in $TC(H)$ have access to all primitive actions directly, 
the convergence to the optimal policy can be guaranteed. 
\begin{theorem}
    Let $TC(H)$ be a total leaf cover for a hierarchy $H$, and $Q^t = Q_r^t + Q_c^t + Q_e^t$.
    If we use equations (\ref{eq:TdQr}-\ref{eq:TdQe}) to update the Q-values for all subtasks in $TC(H)$, 
    we have $lim_{t \rightarrow \infty} Q^t(i, s, a) = Q^*(i, s, a)$
    if the following conditions hold:
    \begin{itemize}{}
    \item Every subtask in $TC(H)$ has access to all primitive actions and can only execute primitive actions
    \item A greedy in the limit with infinite exploration (GLIE) exploration policy is followed by every subtask in $TC(H)$
    \item $Var\{R_t(s' | s, a)\}$ is finite 
    \item $\sum_t \alpha_t = \infty$ and  $\sum_t \alpha_t^2 \le \infty$
    \item $0 < \gamma < 1$ or $\gamma=1$ and all policies $\pi_i$ are proper
    \end{itemize}
    %TODO: cite Neuro-Dynamic Programming 1996
%above learning algorithm will converge (with appropriately
%decaying learning rates and exploration method) to a
%hierarchically optimal policy
 
\end{theorem}
%If we let all nodes which are the parent of some primitive Max nodes to have access
%to all primitive actions, we can construct $\mathbb{C}$ by including all 
%such nodes. 
\textbf{Proof:} 
\begin{align*}
    Q^{t+1}(i, s, a) = &Q_r^{t+1}(i, s, a) + Q_c^{t+1}(i, s, a) + Q_e^{t+1}(i, s, a) \\
                     = &(1 - \alpha_t){Q}_r^{t}(i, s, a) + \alpha_t R_t(s'| s, a) \ +\\
                       &(1 - \alpha_t){Q}_c^{t}(i, s, a) + \alpha_t \gamma^N[{Q}_r^{t}(i', s', a') + {Q}_c^t(i', s', a')] \ +\\
                       &(1 - \alpha_t){Q}_e^{t}(i, s, a) + \alpha_t \gamma^N[{Q}_e^{t}(i', s', a')]\\
                      =&(1 - \alpha_t)[{Q}_r^{t}(i, s, a) + {Q}_c^{t}(i, s, a) + {Q}_e^{t}(i, s, a)] \ +\\
                       &\alpha_t[ R_t(s'| s, a) + \gamma^N[{Q}_r^{t}(i', s', a') + {Q}_c^t(i', s', a') + {Q}_e^{t}(i', s', a')]] \\
                      =&(1 - \alpha_t) {Q}^{t}(i, s, a) + \alpha_t[ R_t(s'| s, a) + \gamma^N  {Q}^{t}(i', s', a') ]
\end{align*}
    Since $a$ and $a'$ are primitive actions, the HORDQ rule identical to the standard Q-learning update rule.
    Therefore, it converges under the same condition as Q-learning. \textbf{Q.E.D.}

One of the problem of hierarchical optimal learning algorithms is that policy $\pi_i$ of subtask $M_i$
is determined by the reward of original MDP. There are no pseudo rewards which are allowed as
in MAXQ. Due to the lack of pseudo reward, each subtask $M_i$ lacks the motivation to 
pursue the subgoal defined by the hierarchy, thus it makes the hierarchical 
design useless. It is necessary to add some pseudo reward to encourage each subtask to pursue 
the subgoal. However, we cannot guarantee optimality with a nonzero pseudo reward.
A practical approach is to use positive reward to encourage the effective exploration
in the early stage, and gradually decrease it to 0. 

%The penalty term serves as a mechanism to enforce the subtask to follow the hierarchy.
%The subtask will strictly follow the subgoal defined by the hierarchy if the penalty term is large.
%On the other hand, if the penalty term is small, the subtask is more likely to go rouge and try to 
%solve the whole problem on its own. Here we have a engineering decision: if we trust our hierarchy design, 
%we should increase the penalty term to let the agent find the optimal policy as fast as it can; 
%if not, a low penalty term allows the agent to find the optimal policy when the hierarchy doesn't work.

%Here we show a way to add the pseudo reward without violating the hierarchical 
%optimal constraint:
%\begin{theorem}
    %Let $x$ be some terminal state of subtask $M_i$, $R(x)$ be the reward
    %when the agent arrives state $x$, $\tilde{R}(x) = R(x) - r$ be the pseudo reward
    %and $r$ be the penalty term.
    %If $P(x| s, \pi_i^*(s)) = 0$, we have $Q^*(i, s, \pi^*(s)) = \tilde{Q}^*(s, \tilde{\pi}(s))$.
%\end{theorem}

%The above theorem says that the optimal policy does not change 
%if some penalty is applied at some terminal states which are not part of the optimal path.


%We need the penalty for the hierarchy to work. 
%Since we do not usually know what the optimal policy is, we may add penalty term in 
%wrong states and lose the optimality. But we do not always require 


%The idea of our work is to use the approximated model-based node to 
%compute the plan for the agent, and let the hierarchical optimal model-free node
%to execute the plan. If everything works a李世光s the plan, the agent would converge to the optimal
%policy in a short time. If not (following the plan is worse than the penalty term),
%the model-free node will take control and find the optimal policy on its own.

\section{Planning with Static Assumptions}
\label{se:Model}

Based on the results of the previous section, we can safely use any approximate model-based method 
to learn the Q-values of subtasks which do not belong to $TC(H)$ without worrying
that it will violate the optimality condition. Model-based RL requires the enumeration of all possible states during the planning process.
However, it is not feasible when the state space is too large.
Previous methods (\cite{ApproxDyna}, \cite{ApproxTree}) rely on function approximation
techniques to predict the next state. Their model is biased because it is possible 
to encounter different next states given the same state and policy for a stochastic problem. If we 
predict one of them, we ignore the stochastic nature of the problem. If we predict
several, the number of states under consideration may grow exponentially with the number of simulating steps
and become intractable after a few steps.

We notice that, for some applications, there are some variables which are more important
than others. Take the mobile robot navigation problem, for example; the location of the robot 
is the key to the planning process, while the movement of other objects in the environment is less important.
Our idea is to separate the variables into planning variables 
and environment variables. During the planning process, we enumerate all possible values of the planning variables, while assuming 
the remaining environment variables are static throughout the process. 
If we limit the number of planning variables to be small, the enumeration process can be efficient.
It also simplifies the learning of the transition and reward functions, since we only need to learn
the dynamics and reward models for the planning variables.

Let state $s = (x, y)$, where $x$ consists of planning variables and $y$ consists of environment
variables. Following the MAXQ approach, we decompose the Q-function as:
\begin{equation}
    Q^{\pi}(i, x, j) = Q_r^{\pi}(i, x, j) + Q_c^{\pi}(i, x, j),
    \label{eq:biasedMaxQ}
\end{equation}
where $Q_r^{\pi}(i, x, j)$ is provided by the child subtask $M_j$.

The task of subtask $M_i$ is to compute $Q_c^{\pi}(i, x, j)$ by:
\begin{equation}
    Q_c^{\pi}(i, x, j) = \sum_{x'} P_m^{\pi}(x'|s, j)[Q_r^{\pi}(i, x', \pi_i(x')) + Q_c^{\pi}(i, x', \pi(x'))],
    \label{eq:biasedQc}
\end{equation}

With the formula above, the Q-values can be computed by dynamic programming.

For simplicity, we use the multi-time model \cite{SMDP} to model the transition function: 
\begin{equation}
    P_m(x|s, j) = \sum^{\infty}_{N=1} \gamma^N P(x, N|s, j).
    \label{eq:multiProb}
\end{equation}

$P_m(x|s, j)$ can be estimated by:
\begin{equation}
    \tilde{P}_m(x|s, j) = (1-\alpha)\tilde{P}_m(x|s, j) + \alpha [ \gamma^N \delta_{x'x}],
    \label{eq:approxP}
\end{equation}
for all $x \in S_i$, where $\delta_{x'x}=1$ if $x' = x$ and is 0 otherwise.

\section{Experimental Results}
%TODO: Say how do we update the model (just for three iteration) with Bellman equation
%TODO: say we use MaxQ for model based approach

\begin{figure}[t]
%\begin{center}
 \begin{minipage}[b]{0.5\linewidth}
    \begin{center}
    %\includegraphics[height=11em, width=6em]{eli_bend.eps}
    \includegraphics[width=2.0in] {./figures/BusSmall.eps}
    %\caption{(a)}
\end{center}
\end{minipage}
\begin{minipage}[b]{0.5\linewidth}
    \begin{center}
    \includegraphics[width=2.0in] {./figures/BusHierarchy.eps}
\end{center}
\end{minipage}
\begin{minipage}[b]{0.5\linewidth} \centering (a) \end{minipage}
\begin{minipage}[b]{0.5\linewidth} \centering (b) \end{minipage}

%\end{center}
\caption{(a) The school bus domain (b) A task graph for the school bus domain.}
\label{fig:bus}
\end{figure}

Figure \ref{fig:bus}(a) shows the school bus domain. The school bus starts at school $S$. Its task 
is to pickup all passengers marked as $P$ and return to the school.
The bus can move North, South, East, or West. There is a reward of -1 applied for each action.
There is 0.1 probability for the bus to move in a random direction. It stays put 
when trying to cross a wall. 
The passengers are picked up automatically when the bus moves into
the location of a passenger. The passengers can be picked up in any order.
The episode ends when it finishes the task.  
There are two possible locations for road construction. They are marked as 
$A$ and $B$. If the bus passes a construction site, it will get damaged with probability 1 and has the probability
of 0.25 to break down for each step afterwards. There is a reward of -50 if the bus breaks down and the episode ends immediately. 
At the beginning of an episode, the status of the road is randomly chosen from no construction, $A$ is under construction,
or $B$ is under construction. There is a 0.05 probability for the road status to change after the episode begins.
The world is divided into six areas. There are six subtasks $Move(1), \dots,$ and $Move(6)$ which move the
bus to the corresponding area.
Subtask $Move(t)$ can only be invoked if $t$ is the adjacent area.
The subtask terminates if the bus exits the current area.
When it terminates, a pseudo-reward $\tilde{r}$ is applied if the bus arrives at designated area $t$ and 0 otherwise.
The task hierarchy is shown in Figure \ref{fig:bus}(b). 

A state can be described by the 8-tuple $(x, y, h, p_1, p_2, p_3, a, b)$, where $(x, y)$ is the location of 
bus, $h$ shows if the bus is damaged, $p_i$ indicates if the corresponding passenger has been picked up or not,
and $a$ and $b$ are binary variables which indicate the status of the construction sites.
The planning variables of our model are $\{x, y, p_1, p_2, p_3\}$, while the damage
status and road conditions are assumed to be static during the planning process.
Our model can learn that a large penalty will be received when the bus is damaged.  
%Since the damage status is assumed static during the planning process, 
However, our model cannot learn that the bus will get damaged if it passes through
a construction site. The model is certainly biased in this case, thus
it cannot learn the optimal policy. The objective of 
the experiment is to show that the optimal policy can still be learned
if we combine the model-based approach with HORDQ. 

Since the six subtasks $Move(1), \dots,$ and $Move(6)$ cover all primitive actions, they form a total leaf cover of the hierarchy.
We used HORDQ in the subtasks to guarantee the convergence to the optimal
policy. Subtask $Root$ adopted our approximate model-based method. 

%The planning variables include 
%the location of bus and the status of passengers. The environment variables are the 
%damage status of bus and the status of road. 
%TODO: total leaf cover always exist
%The world is divided by 6 areas. 

\begin{figure}[t]
%\begin{center}
 \begin{minipage}[b]{0.5\linewidth}
     \begin{center}
    %\includegraphics[height=11em, width=6em]{eli_bend.eps}
    \includegraphics[width=2.5in] {./figures/Approx.eps}
\end{center}
    %\caption{(a)}
\end{minipage}
\begin{minipage}[b]{0.5\linewidth}
     \begin{center}
    \includegraphics[width=2.5in] {./figures/Random.eps}
\end{center}
\end{minipage}
\begin{minipage}[b]{0.5\linewidth} \centering (a) \end{minipage}
\begin{minipage}[b]{0.5\linewidth} \centering (b) \end{minipage}

%\end{center}
\caption{The learning curve for the school bus domain, averaged over 400 episodes. (a) With our model-based approach. (b) With random planner.
The pseudo-reward is shown in parentheses. The parameters are $\alpha=0.1$ and $\gamma=1$. All algorithms follow an $\epsilon$-greedy exploration policy
with $\epsilon = 0.1$.}
\label{fig:res}
\end{figure}

Figure \ref{fig:res}(a) shows the learning curves with different level of pseudo-rewards.
With pseudo-reward +60, it learned a suboptimal policy because
the pseudo-reward is too large to make subtask $Move(t)$ ignore 
the penalty of breakdown. As a result, the subtask followed 
the instruction of its parent too strictly.

On the other hand, if we do not impose any pseudo-reward, 
the optimal policy can be learned, but the learning rate is
slower than SARSA(0) learning. Since the subtask has no
incentive to follow the instruction from the hierarchy, 
the learning process is similar to SARSA(0) learning 
except it has six different Q-functions to learn (one for each subtask) instead of one.
Thus it takes longer to learn the optimal policy. 

With the appropriate pseudo-reward, we can get a near-optimal policy
while the learning rate is faster than SARSA(0)
Our experiment shows that a pseudo-reward of +5 is enough to make the subtask follow 
the order of $Root$ in most of the times, but it is not enough for the subtask to ignore
the breakdown penalty. For example, when $Root$ executes $Move(4)$ to move the bus from area 
3 to area 4 and the road at location $A$ is under construction, $Move(4)$ subtask
will learn it is a bad decision with HORDQ.
Instead of moving to area 4, $Move(4)$ may move to area 1 or 5 to avoid
the breakdown penalty. In turn, $Root$ learns $Move(4)$ cannot be executed in 
such a scenario, thus it will seek an alternate plan if the same scenario
is encountered.

The combination of our approximate model-based approach and MAXQ learns 
a suboptimal policy similar to HORDQ with high pseudo-reward. MAXQ does not estimate the consequence of its action outside its own subtask, therefore
$Move(t)$ will move to area $t$ at any cost.

To simulate the performance of the combination of a poorly-approximated model-based method and 
HORDQ, we replaced our model-based approach with a random policy.
The result is shown in Figure \ref{fig:res}(b). In this case, SARSA(0) has the fastest
learning rate. It takes more time for $Move(t)$ to realize that the policy of $Root$ is bad with higher pseudo rewards.
Nevertheless, it will eventually learn a near-optimal policy.
The combination of random policy and MAXQ has the worst result.

The result shows that a good approximate model 
can help increase the learning rate with the combination of HORDQ. 
If the model is poor, HORDQ serves as a fail-safe mechanism to stop 
the agent repeating the same poor policy over and over again.
On the other hand, MAXQ learned a poor policy in both cases.  
The result suggests that in order to construct a robust HRL 
algorithm, it is beneficial to incorporate HORDQ in the hierarchy.

%On the contrary, the combination of MAXQ and a poor approximated model has
%a performance similar to random policy.

%TODO: add random agent experiment
%TODO: if I set the pseudo reward to zero, will the whole stuff breaks down again?


\endinput
%Most of the previous work on RL focus on model-free approaches for several reasons. 
%The most 
%Within a planning agent, there are at least two roles for real experience: it
%can be used to improve the model (to make it more accurately match the real
%environment) and it can be used to directly improve the value function and
%policy using the kinds of reinforcement learning methods we have discussed in
%previous chapters. The former we call model-learning, and the latter we call
%direct reinforcement learning (direct RL). The possible relationships between
%experience, model, values, and policy are summarized in Figure  9.2. Each arrow
%shows a relationship of influence and presumed improvement. Note how experience
%can improve value and policy functions either directly or indirectly via the
%model. It is the latter, which is sometimes called indirect reinforcement
%learning, that is involved in planning. 

%The second branch is model-based RL, which directly estimates
%a model of the environment and then plans with
%this model. Early work demonstrated that summarizing an
%agent’s experience into a model could be an efficient way
%to reuse data (Moore & Atkeson, 1993), and later work utilized
%the uncertainty in an agent’s model to guide exploration,
%yielding the first (probabilistic) finite bounds on the amount of data required
%to learn near-optimal behaviors in the general case (Kearns & Singh, 1998;
%Kakade, 2003).


%motivation: play a randomly generate maze to adapt to
%adapt to noval situation

%weakness-> an adversary to block the path forever
%no knowledge (coin won't disappear-->need hack to do it, moster won't move)
%comparison to the previous work->HRL,


%model based-> state space is too large. good to train with
%few examples (10 sec training)
%model free-> cannot handle randomly generated maze -> failed to adapt to noval situation. can handle large space (linear SARSA)
%Experiment: comparison with flat model based approach and hierarchical model based approach with approximation

%combine both-> use model based on top level to reduce
%the state space, and model free on bottom to 
%deal with randomized noisy world for short term reward

%build multilevel of hierachy on features
%label each room with a number 1, 2, 3
%the coordinate of wrt the room
%(1, (25, 30))
%move from room 1 to room 2
%top level
%action (1->2)
%second level
%assume (1, (25, 30)) -> (2, (0, 0))
%action ((25, 30) -> (0, 0)) goal (-25, -30) in single step

%need to build time model of the full state
%P(s->s'|x, t) the propability to move from state s to s'
%after t steps of the observation of full state x

%power coms from trasition model (model the shortest path), not shortest path
%you can use model-based approach on this

%fickle passenger of MaxQ
%coarseness on time and space scale

%-->you can work on continus varible now, no more grid world

%1. Assumption on the state difference (if it is not true --> like the agent in a world boundary or the health is 100 and cannot be imporve
  %since the state is outside the known state boundary, the planner whon't plan for it. but the local planner may still direct the agent
  %to go outside the world boundary, which may be a problem)
%2. Application to the hierachical RL
%3. Limitation: unachievable state (a coin surrounded by many monsters)
%4. Can inference in a very small world, not need to do 64 by 64
%Sometimes we need a plan. The greedy approach of reinforcement learning does not always work. 
%RL techniques have several limitations:
%1. It's difficult for to transfer the value function from a small world to a large one. If the agent has
%the experience only in a small world like 4 \times 4 grid, it cannot act well in 64 \times 64 grid
%because it does not know the correct action in regard to an object 63 grid away.

%2. It takes a long time to train the agent to be good enough. The greedy 

%3. The approximation only good in a small range. We need a plan for long term action.
%The noval state (the state in current game which has not been experienced) may 

%4. Impossible task to maze problems: RL require that optimal agent to find the optimal solution for a maze when all the features
%are input to the agent. Which is unlikely to work.(impractical when the maze is large)(unable to transfer
%the knowledge from previous maze experience to the current one) A practical solver shall invovle planning through 
%the possible route and find the one which can lead to the exist.
%All the work requires the agent to repeat play the same maze (assuming traps) until it finds the optimal solution.
%What if the maze changes every time?

%1. Is it guarranteed convergence? Yes, if we choose the goal next to the current state which leads to highest Q value. 
%It is the same as SARSA.
%2. How to choose the goal to guarrante convergence to optimal policy?

%Good to work on the problem with a long term reinforcement (feedback) like a maze.
%Bad to work with a problem with dynamic envirment (everything changes with or without agents actions)
%Bad when the consequence of an action is delayed for many steps. (poison)

%Q: prove that RL cannot solve maze problem
%Ability to transfer the knowledge from one maze to another
%compare the HRL in key-finding problem
%compare to the model-based RL





%application: the key-room problem--> each room lock a key, one room has a treasure, the agent needs to go through 
%a maze to collect the keys for each door and get the treasure

%Stochastic Shortest-Path Problems
%A Stochastic Shortest-Path problem is an mdp prob-
%lem in which the state space S = f1; : : : ; n; tg is such
%that t is a goal (target) state that is absorbing (i.e.,
%p(t; u; t) = 1 and g(t; u) = 0 for all u 2 U(t)), and the
%discount factor ® = 1. In this case, the existence of
%optimal policies (and optimal stationary policies) is a
%major mathematical problem. However, the existence
%is guarantee under the following reasonable conditions:
%(A1) There exists a policy that achieves the goal with
%probability 1 from any starting state.
%(A2) All costs are positive.
%The ¯rst assumption just expresses the fact that the
%problem admits a well-behaved solution. Such policies
%are known as proper policies. The second assumption,
%in the other hand, guarantees that all improper policies
%incurs in in¯nite cost for at least one state. Thus, both
%assumptions preclude cases where the optimal solution
%might \wander" around without never getting to the
%goal. For example, a problem having a zero-cost cycle
%(in state space) violates the second assumption.
%As mentioned in the Introduction, often we are only
%interested in knowing how to go from a ¯xed initial
%state, say 1, to the goal state. The optimal solution in
%this case is an partial optimal stationary policy ¹ such
%that ¹(i) = ¹¤(i) for all states i that are reachable
%from 1 when using the optimal policy ¹¤; the so-called
%relevant states when starting from 1.1
%Finding a partial optimal policy can be consider-
%ably simpler, the extreme case when the set of relevant
%states is ¯nite and the complete state space is in¯nite.
%Thus, the question of how to ¯nd partial optimal poli-
%cies is of great relevance. One algorithm for that is
%Real-Time Dynamic Programming.

%There are two ways for a RL agent to use its sample data. It can use the sample data to update the 
%value function and improve its policy. It is called model-free (or direct) reinforcement learning. 
%Another approach is to use the sample data to estimate parameters for the model of the environment and compute the value function
%from the model. It is called model-based (indirect) reinforcement learning.

%The advantage of model-based approaches is that they make more efficient use of the sample data, thus 
%it take fewer time to train a model-based RL agent. Other the other hand, model-free approaches do not assume 
%a prior model, so it would not be affected by the bias of the model design. Besides, the existence of 
%good linear approximation algorithms, such as linear SARSA, makes it possible for model-free approaches to
%handle large scale problems.

%Model-based approaches needs to enumerate through all possible states to compute the value function. 
%But the number of states grows exponentially with the number of features, 
%and it quickly becomes intractable if the number features grows large.

%The main idea of this work is to combine the model-free, model-based and hierarchical reinforcement learning.
%For the lower level of hierarchy, we adopt the model-free approach to handle the complete and possibly large state space.
%On the top level of hierarchy, we use the model-based to plan on a coarser level, which contains only small number of states.
%This approach allows the agent to plan on the higher level and make efficient use of the sample data. On the lower level,
%the agent uses model-free approaches with linear approximation to handle the complete state space.

%The prior works on hierarchical reinforcement learning (HRL) can be divided into two categories: model-free and model-based approaches. 
%The model-free approaches include HAMQ \cite{HAMQ}, MAXQ \cite{MAXQ}, SMDP and options framework \cite{option}
%For model-based approaches, Seri et al. \cite{HLearning} proposed a model-based HRL in average reward setting.
%Diuk et al. \cite{Diuk} adopted model-based HRL to deterministic domain. Jong and Stone \cite{RMaxQ} combined 
%R-MAXQ and MAXQ to achieve efficient model-based exploration and the action abstraction of MAXQ.

%TODO: show that model based state difference can learn randomly generated maze with very limited steps (only four touches of the wall)
%TODO: use a poison example to show the limit of pure approximated model-based approach and the motivation
%of use hierarchical optimal policy for pseudo reward and recursive optimal policy for real reward. It can be shown that
%it will converge to optimal policy if the true goal state of the game is included as a subgoal(
%the worst case is the child node finish the game by itself)
